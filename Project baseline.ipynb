{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as td\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicTextsDataset(td.Dataset):\n",
    "    def __init__(self, data_path='train.csv', \n",
    "                       n_train_batches=16000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.2,\n",
    "                       valid_size=0.1,\n",
    "                       batch_size=10, \n",
    "                       vocab_size=2000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation from all data or from separated \n",
    "                                      parts of data (a chance of intersection between training, testing and validation data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating dataset for testing. \n",
    "                        Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating dataset for validation. \n",
    "                        Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] most frequently encountered\n",
    "                         tokens, all others will be encoded as UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset will be returned \n",
    "                    sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDataset, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2:])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with corresponding labels).\n",
    "           Produced dataset is drawn with repetition from initial data, and therefore can contain duplicates\n",
    "           Depending on n_batches, it will do either undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with classification labels\n",
    "                        (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples in dataset is \n",
    "                             equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch of inputs and \n",
    "                            dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        masks = self.MakeMasks(y)\n",
    "        n_subbatches = n_batches // len(masks)\n",
    "        \n",
    "        if(self.verbose >= 2): print('\\n')\n",
    "        \n",
    "        for mask in masks:\n",
    "            if(self.verbose >= 2): print('\\tApplying mask: ' + mask['name'] + '... ', end='')\n",
    "            dataset += self.CreateDatasetFromXY(X[mask['mask']], y[mask['mask']], n_subbatches)\n",
    "            if(self.verbose >= 2): print('Completed')\n",
    "                \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples in dataset is \n",
    "                             equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch of inputs and \n",
    "                            dataset[i]['labels'] - corresponding batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and place texts of the same \n",
    "        # lenght in the same position in this dictionary. This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        for i in range(len(X)):\n",
    "            codes = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more samples of lenght ~10-50 tokens and \n",
    "        # much smaller number of samples of lenght 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of lists and numpy array to torch tensor :)\n",
    "            batch = {'input':Variable(torch.LongTensor(np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist()))),\n",
    "                     'labels':Variable(torch.FloatTensor(np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())))}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with classification labels\n",
    "                        (label != 0 is assumed to be \"interesting\" - we prioretize tokens encoundered in examples with \n",
    "                        at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[np.sum(y, axis=1) > 0] \n",
    "        X_relevant += shuffle(X[np.sum(y, axis=1) == 0])[:len(X_relevant)] # we add random portion of \"all-negative\" \n",
    "                                                                            # data of equal size \n",
    "         \n",
    "        for text in X_relevant:\n",
    "            tokens = self.Smart_Split(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "        out = text.strip().lower().replace('\\n', ' ')\n",
    "        out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "        out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "        return out.split()\n",
    "\n",
    "    def MakeMasks(self, y):\n",
    "        \"\"\"this function makes masks (bool np.arrays of length y). Each mask is cunstructed so that X[mask] is a part of \n",
    "        data grouped by some combination of labels (for example - all data with al labels = 0, or all data with\n",
    "        first class label = 1 and all other equal to 0, or all data with all labels equal to 1)\n",
    "            INPUT: y - np.array of shape [n_samples, n_classes]\n",
    "            OUTPUT: masks - list of bool np.arrays of length y\n",
    "        \"\"\"\n",
    "        \n",
    "        def not_i_col(y, i):\n",
    "            \"\"\"Utility function that returns all columns of y, except i-th\"\"\"\n",
    "            mask = np.array([True, True, True, True, True, True])\n",
    "            mask[i] = False\n",
    "            return y[:, mask]\n",
    "\n",
    "        # mask for data with label_excluded_i = 1 and all others = 0\n",
    "        # important: there is no data for label_1 = 1 and all others equal to 0, so skipping that mask\n",
    "        mask1 = []\n",
    "        for excluded_i in range(6):\n",
    "            mask1.append(np.logical_and(y[:, excluded_i] == 1, np.sum(not_i_col(y, excluded_i), axis=1) == 0))\n",
    "\n",
    "        # masks for 2, 3, 4, 5 and 6 labels respectivly equal to 1 (here we do not care, which label (i.e. label_1, label_2, ...) \n",
    "        # is equal to 1, just that there is exactly n=2,3,.. labels equal to 1)\n",
    "        mask2 = np.sum(y, axis=1) == 2\n",
    "        mask3 = np.sum(y, axis=1) == 3\n",
    "        mask4 = np.sum(y, axis=1) == 4\n",
    "        mask5 = np.sum(y, axis=1) == 5\n",
    "        mask6 = np.sum(y, axis=1) == 6\n",
    "\n",
    "        mask0 = (np.sum(y, axis=1) == 0)\n",
    "\n",
    "        # let's save all masks in one list:\n",
    "        masks = [{'mask':mask0, 'name':'all-negative data'}, \n",
    "                 {'mask':mask1[0], 'name':'only fisrt class labeled positive'},\n",
    "                 {'mask':mask1[2], 'name':'only third class labeled positive'},\n",
    "                 {'mask':mask1[3], 'name':'only fourth class labeled positive'},\n",
    "                 {'mask':mask1[4], 'name':'only fifth class labeled positive'},\n",
    "                 {'mask':mask1[5], 'name':'only sixth class labeled positive'},\n",
    "                 {'mask':mask2, 'name':'exactly two positive labels'},\n",
    "                 {'mask':mask3, 'name':'exactly three positive labels'},\n",
    "                 {'mask':mask4, 'name':'exactly four positive labels'},\n",
    "                 {'mask':mask5, 'name':'exactly five positive labels'},\n",
    "                 {'mask':mask6, 'name':'all-positive data'}]\n",
    "            \n",
    "        if(self.verbose >= 2): print('\\n\\tMasks created (a reminder - no data for \"only second class labeled positive\")', end='')\n",
    "        \n",
    "        return masks\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=20000, embedding_dim = 100, hidden_dim=64, batch_size=10, conv_channels=32):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "                                     # // 2 is because we would like to concat hidden states, \n",
    "                                    # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "#         self.attention = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 0))\n",
    "#         )\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=conv_channels, kernel_size=5, padding=2)\n",
    "    \n",
    "        self.linear = nn.Linear(conv_channels, 1)\n",
    "        self.linear_final = nn.Linear(hidden_dim, 6) # we have 6 classes to predict\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        self.hidden = (Variable(torch.zeros(2, self.batch_size, self.hidden_dim // 2)), \n",
    "                       Variable(torch.zeros(2, self.batch_size, self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        embed = self.embeddings(input_seq)\n",
    "        output, _ = self.lstm(embed, self.hidden)\n",
    "        \n",
    "        conv_out = self.conv(embed.resize(embed.data.shape[0], embed.data.shape[2], embed.data.shape[1]))\n",
    "        \n",
    "        attention_tensor = torch.mean(conv_out, dim=1)\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        \n",
    "        predictions = nn.functional.sigmoid(self.linear_final(features))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from train.csv... Completed\n",
      "Generating vocabulary... Completed\n",
      "Creating train dataset... \n",
      "\tMasks created (a reminder - no data for \"only second class labeled positive\")\n",
      "\n",
      "\tApplying mask: all-negative data... Completed\n",
      "\tApplying mask: only fisrt class labeled positive... Completed\n",
      "\tApplying mask: only third class labeled positive... Completed\n",
      "\tApplying mask: only fourth class labeled positive... Completed\n",
      "\tApplying mask: only fifth class labeled positive... Completed\n",
      "\tApplying mask: only sixth class labeled positive... Completed\n",
      "\tApplying mask: exactly two positive labels... Completed\n",
      "\tApplying mask: exactly three positive labels... Completed\n",
      "\tApplying mask: exactly four positive labels... Completed\n",
      "\tApplying mask: exactly five positive labels... Completed\n",
      "\tApplying mask: all-positive data... Completed\n",
      "Completed\n",
      "Creating validation dataset... \n",
      "\tMasks created (a reminder - no data for \"only second class labeled positive\")\n",
      "\n",
      "\tApplying mask: all-negative data... Completed\n",
      "\tApplying mask: only fisrt class labeled positive... Completed\n",
      "\tApplying mask: only third class labeled positive... Completed\n",
      "\tApplying mask: only fourth class labeled positive... Completed\n",
      "\tApplying mask: only fifth class labeled positive... Completed\n",
      "\tApplying mask: only sixth class labeled positive... Completed\n",
      "\tApplying mask: exactly two positive labels... Completed\n",
      "\tApplying mask: exactly three positive labels... Completed\n",
      "\tApplying mask: exactly four positive labels... Completed\n",
      "\tApplying mask: exactly five positive labels... Completed\n",
      "\tApplying mask: all-positive data... Completed\n",
      "Completed\n",
      "=========================================\n",
      "Start of the training.\n",
      "Epoch 000; train loss = 350.90; validation loss = 135.65; validation F1 score = 0.29; ETA = 1825 s\n",
      "Epoch 001; train loss = 331.20; validation loss = 133.90; validation F1 score = 0.51; ETA = 1713 s\n",
      "Epoch 002; train loss = 320.98; validation loss = 133.20; validation F1 score = 0.54; ETA = 1619 s\n",
      "Epoch 003; train loss = 318.79; validation loss = 130.61; validation F1 score = 0.48; ETA = 1561 s\n",
      "Epoch 004; train loss = 316.96; validation loss = 130.50; validation F1 score = 0.44; ETA = 1469 s\n",
      "Epoch 005; train loss = 312.35; validation loss = 132.14; validation F1 score = 0.55; ETA = 1384 s\n",
      "Epoch 006; train loss = 318.64; validation loss = 130.24; validation F1 score = 0.53; ETA = 1282 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007; train loss = 310.71; validation loss = 130.59; validation F1 score = 0.50; ETA = 1175 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b65353c17f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4b894b947295>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mconv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         )\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 5 # big batch sizes are not recomended, \n",
    "               # since a lot of batches have 1 or 2 samples, repeated batch_size times\n",
    "               # for now a batch_size of 5 to 15 seems reasonable\n",
    "\n",
    "model = LSTMClassifier(batch_size=batch_size)\n",
    "dataset = ToxicTextsDataset(n_train_batches=500, \n",
    "                            n_test_batches=50, \n",
    "                            n_valid_batches=200,\n",
    "                            valid_size=0.3,\n",
    "                            test_size=0.,\n",
    "                            batch_size=batch_size, \n",
    "                            vocab_size=5000, \n",
    "                            verbose=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_function = nn.MultiLabelSoftMarginLoss() #weight=torch.FloatTensor(dataset.class_reversed_weights))\n",
    "\n",
    "losses = [0]\n",
    "valid_loss=[0]\n",
    "\n",
    "print('=========================================')\n",
    "print(\"Start of the training.\")\n",
    "start = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    all_predictions = np.array([0, 0, 0, 0, 0, 0])\n",
    "    all_true_labels = np.array([0, 0, 0, 0, 0, 0])\n",
    "    for mode in ['train', 'valid']:\n",
    "        dataset.mode = mode\n",
    "        dataset.shuffle()\n",
    "        for sample in dataset:\n",
    "            if(mode == 'train'):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                model.init_hidden()\n",
    "                pred = model.forward(sample['input'])\n",
    "\n",
    "                loss = loss_function(pred, sample['labels'])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses[-1] += loss.data[0]\n",
    "            else:\n",
    "                model.init_hidden()\n",
    "                pred = model.forward(sample['input'])\n",
    "                valid_loss[-1] += loss_function(pred, sample['labels']).data[0]\n",
    "                all_predictions = np.vstack((all_predictions, pred.data.numpy()))\n",
    "                all_true_labels = np.vstack((all_true_labels, sample['labels'].data.numpy()))\n",
    "                \n",
    "    all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "    print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; validation F1 score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "                                                                    losses[-1], valid_loss[-1],\n",
    "                                                                    f1_score(all_true_labels, all_predictions, average='weighted'),\n",
    "                                                                    (epochs - i)*(time.time() - start)/(i+1)))\n",
    "    losses.append(0)\n",
    "    valid_loss.append(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
