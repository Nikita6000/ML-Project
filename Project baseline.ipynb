{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as td\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicTextsDataset(td.Dataset):\n",
    "    def __init__(self, data_path='train.csv', \n",
    "                       n_train_batches=16000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.2,\n",
    "                       valid_size=0.1,\n",
    "                       batch_size=10, \n",
    "                       vocab_size=2000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0,\n",
    "                       use_cuda = True):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation \n",
    "                                      from all data or from separated parts of data (a chance \n",
    "                                      of intersection between training, testing and validation \n",
    "                                      data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                        dataset for testing. Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                         dataset for validation. Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] \n",
    "                         most frequently encountered tokens, all others will be encoded as \n",
    "                         UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset \n",
    "                    will be returned sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDataset, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2:])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, \n",
    "                                                    test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for \n",
    "            each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers, \n",
    "                        cl_features - list of floats (character level features)\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        cl_features = self.ComputeCharacterLevelFeatures(text)\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes, cl_features\n",
    "    \n",
    "    def ComputeCharacterLevelFeatures(self, text):\n",
    "        \"\"\"This function computes a character level features \n",
    "           INPUT: text - a python string\n",
    "           OUTPUT: cl_features - a list of floats\n",
    "               \n",
    "               cl_features[0] - lenght of text\n",
    "               cl_features[1] - mean of lenghts of all tokens in text\n",
    "               cl_features[2] - ratio of capital letters in text\n",
    "               cl_features[3] - ratio of non-letter symbols in text\n",
    "        \"\"\"\n",
    "        text_len = float(len(text))\n",
    "        \n",
    "        cl_features = [\n",
    "            text_len,\n",
    "            np.mean([len(token) for token in self.Smart_Split(text)]),\n",
    "            len(re.findall(r'[A-Z]', text)) / text_len,\n",
    "            (1. - len(re.findall(r'[a-zA-Z]', text)) / text_len)\n",
    "        ]\n",
    "        \n",
    "        return cl_features\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with \n",
    "           corresponding labels). Produced dataset is drawn with repetition from initial data, \n",
    "           and therefore can contain duplicates Depending on n_batches, it will do either \n",
    "           undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text \n",
    "                     as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples \n",
    "                             in dataset is equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch \n",
    "                            of inputs and dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        masks = self.MakeMasks(y)\n",
    "        n_subbatches = n_batches // len(masks)\n",
    "        \n",
    "        if(self.verbose >= 2): print('\\n')\n",
    "        \n",
    "        for mask in masks:\n",
    "            if(self.verbose >= 2): print('\\tApplying mask: ' + mask['name'] + '... ', end='')\n",
    "            dataset += self.CreateDatasetFromXY(X[mask['mask']], y[mask['mask']], n_subbatches)\n",
    "            if(self.verbose >= 2): print('Completed')\n",
    "                \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples \n",
    "        with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                     text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number \n",
    "                             of samples in dataset is equal to n_batches * batch_size = \n",
    "                             len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th \n",
    "                            batch of inputs and dataset[i]['labels'] - corresponding \n",
    "                            batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and \n",
    "        # place texts of the same lenght in the same position in this dictionary. \n",
    "        # This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        for i in range(len(X)):\n",
    "            codes, cl_features = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, cl_features, y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, cl_features, y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more \n",
    "        # samples of lenght ~10-50 tokens and much smaller number of samples of lenght \n",
    "        # 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of \n",
    "            # lists and numpy array to torch tensor :)\n",
    "            if(self.use_cuda):\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False).cuda()}\n",
    "            else:\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False)}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                       text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                       classification labels (label != 0 is assumed to be \"interesting\" - \n",
    "                       we prioretize tokens encoundered in examples with at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[np.sum(y, axis=1) > 0] \n",
    "        X_relevant += shuffle(X[np.sum(y, axis=1) == 0])[:len(X_relevant)] \n",
    "        # we add random portion of \"all-negative\" data of equal size \n",
    "         \n",
    "        for text in X_relevant:\n",
    "            tokens = self.Smart_Split(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly \n",
    "        # classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the \n",
    "        # same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "        out = text.strip().lower().replace('\\n', ' ')\n",
    "        out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "        out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "        out = out.replace('.  .  .', '...')\n",
    "        return out.split()\n",
    "\n",
    "    def MakeMasks(self, y):\n",
    "        \"\"\"this function makes masks (bool np.arrays of length y). Each mask is \n",
    "        cunstructed so that X[mask] is a part of data grouped by some combination \n",
    "        of labels (for example - all data with al labels = 0, or all data with\n",
    "        first class label = 1 and all other equal to 0, or all data with all \n",
    "        labels equal to 1)\n",
    "            INPUT: y - np.array of shape [n_samples, n_classes]\n",
    "            OUTPUT: masks - list of bool np.arrays of length y\n",
    "        \"\"\"\n",
    "        \n",
    "        def not_i_col(y, i):\n",
    "            \"\"\"Utility function that returns all columns of y, except i-th\"\"\"\n",
    "            mask = np.array([True, True, True, True, True, True])\n",
    "            mask[i] = False\n",
    "            return y[:, mask]\n",
    "\n",
    "        # mask for data with label_excluded_i = 1 and all others = 0\n",
    "        # important: there is no data for label_1 = 1 and all others equal to 0, \n",
    "        # so skipping that mask\n",
    "        mask1 = []\n",
    "        for excluded_i in range(6):\n",
    "            mask1.append(np.logical_and(y[:, excluded_i] == 1, \n",
    "                                        np.sum(not_i_col(y, excluded_i), axis=1) == 0))\n",
    "\n",
    "        # masks for 2, 3, 4, 5 and 6 labels respectivly equal to 1 (here we do not care, \n",
    "        # which label (i.e. label_1, label_2, ...) \n",
    "        # is equal to 1, just that there is exactly n=2,3,.. labels equal to 1)\n",
    "        mask2 = np.sum(y, axis=1) == 2\n",
    "        mask3 = np.sum(y, axis=1) == 3\n",
    "        mask4 = np.sum(y, axis=1) == 4\n",
    "        mask5 = np.sum(y, axis=1) == 5\n",
    "        mask6 = np.sum(y, axis=1) == 6\n",
    "\n",
    "        mask0 = (np.sum(y, axis=1) == 0)\n",
    "\n",
    "        # let's save all masks in one list:\n",
    "        masks = [{'mask':mask0, 'name':'all-negative data'}, \n",
    "                 {'mask':mask1[0], 'name':'only fisrt class labeled positive'},\n",
    "                 {'mask':mask1[2], 'name':'only third class labeled positive'},\n",
    "                 {'mask':mask1[3], 'name':'only fourth class labeled positive'},\n",
    "                 {'mask':mask1[4], 'name':'only fifth class labeled positive'},\n",
    "                 {'mask':mask1[5], 'name':'only sixth class labeled positive'},\n",
    "                 {'mask':mask2, 'name':'exactly two positive labels'},\n",
    "                 {'mask':mask3, 'name':'exactly three positive labels'},\n",
    "                 {'mask':mask4, 'name':'exactly four positive labels'},\n",
    "                 {'mask':mask5, 'name':'exactly five positive labels'},\n",
    "                 {'mask':mask6, 'name':'all-positive data'}]\n",
    "            \n",
    "        if(self.verbose >= 2): print('\\n\\tMasks created (a reminder - no data for \"only second class labeled positive\")', end='')\n",
    "        \n",
    "        return masks\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicTextsDatasetBinary(td.Dataset):\n",
    "    def __init__(self, label_index,\n",
    "                       data_path='train.csv', \n",
    "                       n_train_batches=4000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.,\n",
    "                       valid_size=0.3,\n",
    "                       batch_size=6, \n",
    "                       vocab_size=2000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0,\n",
    "                       use_cuda = True):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation \n",
    "                                      from all data or from separated parts of data (a chance \n",
    "                                      of intersection between training, testing and validation \n",
    "                                      data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                        dataset for testing. Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                         dataset for validation. Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] \n",
    "                         most frequently encountered tokens, all others will be encoded as \n",
    "                         UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset \n",
    "                    will be returned sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDatasetBinary, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2+label_index])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, \n",
    "                                                    test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for \n",
    "            each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers, \n",
    "                        cl_features - list of floats (character level features)\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        cl_features = self.ComputeCharacterLevelFeatures(text)\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes, cl_features\n",
    "    \n",
    "    def ComputeCharacterLevelFeatures(self, text):\n",
    "        \"\"\"This function computes a character level features \n",
    "           INPUT: text - a python string\n",
    "           OUTPUT: cl_features - a list of floats\n",
    "               \n",
    "               cl_features[0] - lenght of text\n",
    "               cl_features[1] - mean of lenghts of all tokens in text\n",
    "               cl_features[2] - ratio of capital letters in text\n",
    "               cl_features[3] - ratio of non-letter symbols in text\n",
    "        \"\"\"\n",
    "        text_len = float(len(text))\n",
    "        \n",
    "        cl_features = [\n",
    "            text_len,\n",
    "            np.mean([len(token) for token in self.Smart_Split(text)]),\n",
    "            len(re.findall(r'[A-Z]', text)) / text_len,\n",
    "            (1. - len(re.findall(r'[a-zA-Z]', text)) / text_len)\n",
    "        ]\n",
    "        \n",
    "        return cl_features\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with \n",
    "           corresponding labels). Produced dataset is drawn with repetition from initial data, \n",
    "           and therefore can contain duplicates Depending on n_batches, it will do either \n",
    "           undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text \n",
    "                     as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples \n",
    "                             in dataset is equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch \n",
    "                            of inputs and dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        n_subbatches = n_batches // 2\n",
    "        \n",
    "        mask = (y == 1)\n",
    "        dataset += self.CreateDatasetFromXY(X[mask], y[mask], n_subbatches)\n",
    "        \n",
    "        mask = (y == 0)\n",
    "        dataset += self.CreateDatasetFromXY(X[mask], y[mask], n_subbatches)\n",
    "        \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples \n",
    "        with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                     text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number \n",
    "                             of samples in dataset is equal to n_batches * batch_size = \n",
    "                             len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th \n",
    "                            batch of inputs and dataset[i]['labels'] - corresponding \n",
    "                            batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and \n",
    "        # place texts of the same lenght in the same position in this dictionary. \n",
    "        # This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        for i in range(len(X)):\n",
    "            codes, cl_features = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, cl_features, y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, cl_features, y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more \n",
    "        # samples of lenght ~10-50 tokens and much smaller number of samples of lenght \n",
    "        # 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of \n",
    "            # lists and numpy array to torch tensor :)\n",
    "            if(self.use_cuda):\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False).cuda()}\n",
    "            else:\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False)}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                       text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                       classification labels (label != 0 is assumed to be \"interesting\" - \n",
    "                       we prioretize tokens encoundered in examples with at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[y == 1] \n",
    "        X_relevant += shuffle(X[y == 0])[:len(X_relevant)] \n",
    "        # we add random portion of \"all-negative\" data of equal size \n",
    "         \n",
    "        for text in X_relevant:\n",
    "            tokens = self.Smart_Split(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly \n",
    "        # classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the \n",
    "        # same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "        out = text.strip().lower().replace('\\n', ' ')\n",
    "        out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "        out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "        out = out.replace('.  .  .', '...')\n",
    "        return out.split()\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=20000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=64, \n",
    "                 batch_size=10, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        \"\"\"\n",
    "            A model from paper \"A Convolutional Attention Model for Text Classification\" \n",
    "            by Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He \n",
    "            http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf\n",
    "            With modified outter layer (softmax -> sigmoid) for multilabel classification\n",
    "            and added character level features\n",
    "            \n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_of_cl_features = num_of_cl_features\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim).cuda()\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
    "                 # // 2 is because we would like to concat hidden states, \n",
    "                # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2).cuda()\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1).cuda()\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 6).cuda() \n",
    "            # we have 6 classes to predict\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2)\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1)\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 6) # we have 6 classes to predict\n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "#         self.attention = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 0))\n",
    "#         )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if(self.use_cuda):\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda(), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq, cl_features=None):\n",
    "        embed = self.embeddings(input_seq)\n",
    "        output, _ = self.lstm(embed, self.hidden)\n",
    "        \n",
    "        conv_out = self.conv(embed.permute(0, 2, 1))\n",
    "        \n",
    "        attention_tensor = torch.mean(conv_out, dim=1)\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        \n",
    "        if(cl_features is not None and self.num_of_cl_features == cl_features.data.shape[1]):\n",
    "            features = torch.cat((features, cl_features), dim=1)\n",
    "        elif(cl_features is not None and self.num_of_cl_features != cl_features.data.shape[1]):\n",
    "            print(\"\"\"Recieved unexpected number of character level features. \n",
    "                     Model expected to recieve {} features, but received {}. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features, cl_features.data.shape[1]))\n",
    "            raise ValueError()\n",
    "        elif(cl_features is None and self.num_of_cl_features > 0):\n",
    "            print(\"\"\"Model expected to recieve {} features, but received None. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features))\n",
    "            raise ValueError()\n",
    "            \n",
    "        predictions = nn.functional.sigmoid(self.linear_final(features))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      " 33%|███▎      | 1/3 [35:57<1:11:55, 2157.66s/it]/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      " 67%|██████▋   | 2/3 [1:13:49<36:54, 2214.73s/it]/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/use/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "100%|██████████| 3/3 [1:51:49<00:00, 2236.57s/it]\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "cross_validation = 3\n",
    "\n",
    "vocab_sizes = [3000, 5000, 7000]\n",
    "embedding_dim = 200\n",
    "hidden_dim = 100\n",
    "conv_channels = 32\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 6 # big batch sizes are not recomended, \n",
    "               # since a lot of batches have 1 or 2 samples, repeated batch_size times.\n",
    "               # for now a batch_size of 5 to 15 seems reasonable\n",
    "use_cuda = True\n",
    "\n",
    "train_stats = []\n",
    "\n",
    "for vocab_size in tqdm.tqdm(vocab_sizes):\n",
    "    train_stats.append({'vocab_sizes':vocab_size, 'train_losses':[], 'valid_losses':[], 'val_f1_scores':[]})\n",
    "    \n",
    "    for _ in range(cross_validation):\n",
    "\n",
    "        dataset = ToxicTextsDataset(n_train_batches=3000, \n",
    "                                    n_test_batches=50, \n",
    "                                    n_valid_batches=1000,\n",
    "                                    valid_size=0.3,\n",
    "                                    test_size=0.,\n",
    "                                    batch_size=batch_size, \n",
    "                                    vocab_size=vocab_size, \n",
    "                                    verbose=0,\n",
    "                                    use_cuda = use_cuda)\n",
    "\n",
    "        Multiple_gpus = False\n",
    "\n",
    "        model = LSTMClassifier(vocab_size=vocab_size, \n",
    "                               embedding_dim = embedding_dim, \n",
    "                               hidden_dim=hidden_dim, \n",
    "                               conv_channels=conv_channels,\n",
    "                               batch_size=batch_size, \n",
    "                               use_cuda=use_cuda)\n",
    "\n",
    "        # todo:\n",
    "\n",
    "        # if (Multiple_gpus and torch.cuda.device_count() > 1):\n",
    "        #     print(\"Detected {} gpu's. Using {} of them.\".format(torch.cuda.device_count(), torch.cuda.device_count()))\n",
    "        #     model.num_gpus = torch.cuda.device_count()\n",
    "        #     model = nn.DataParallel(model, dim=0)\n",
    "        # else:\n",
    "        #     Multiple_gpus = False\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if(use_cuda): loss_function = nn.MultiLabelSoftMarginLoss().cuda()\n",
    "        else: loss_function = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "        train_stats[-1]['train_losses'].append([0])\n",
    "        train_stats[-1]['valid_losses'].append([0])\n",
    "        train_stats[-1]['val_f1_scores'].append([0])\n",
    "\n",
    "#         print('=========================================')\n",
    "#         print(\"Start of the training.\")\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            all_predictions = Variable(torch.zeros(1, 6))\n",
    "            all_true_labels = Variable(torch.zeros(1, 6))\n",
    "\n",
    "            for mode in ['train', 'valid']:\n",
    "                dataset.mode = mode\n",
    "                dataset.shuffle()\n",
    "                for sample in dataset:\n",
    "                    if(mode == 'train'):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if(Multiple_gpus):\n",
    "                            model.module.init_hidden()\n",
    "                        else:\n",
    "                            model.init_hidden()\n",
    "\n",
    "                        pred = model.forward(sample['input'], sample['cl_features'])\n",
    "\n",
    "                        loss = loss_function(pred, sample['labels'])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_stats[-1]['train_losses'][-1][-1] += loss.data[0]\n",
    "                    else:\n",
    "                        if(Multiple_gpus):\n",
    "                            model.module.init_hidden()\n",
    "                        else:\n",
    "                            model.init_hidden()\n",
    "\n",
    "                        pred = model.forward(sample['input'], sample['cl_features'])\n",
    "                        train_stats[-1]['valid_losses'][-1][-1] += loss_function(pred, sample['labels']).data[0]\n",
    "\n",
    "                        all_predictions = torch.cat((all_predictions, pred.cpu()))\n",
    "                        all_true_labels = torch.cat((all_true_labels, sample['labels'].cpu()))\n",
    "\n",
    "\n",
    "            all_predictions = all_predictions.data.numpy()\n",
    "            all_true_labels = all_true_labels.data.numpy()\n",
    "\n",
    "            all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "\n",
    "            train_stats[-1]['val_f1_scores'][-1][-1] = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "            \n",
    "            \n",
    "#             print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; validation F1 score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "#                                                                              train_stats[-1]['train_losses'][-1][-1], \n",
    "#                                                                              train_stats[-1]['valid_losses'][-1][-1], \n",
    "#                                                                              train_stats[-1]['val_f1_scores'][-1][-1],\n",
    "#                                                                             (epochs - i)*(time.time() - start)/(i+1)))\n",
    "            train_stats[-1]['train_losses'][-1].append(0)\n",
    "            train_stats[-1]['valid_losses'][-1].append(0)\n",
    "            train_stats[-1]['val_f1_scores'][-1].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for vs, s in zip(vocab_sizes, train_stats):\n",
    "    data.append([vs, np.mean(np.array(s['val_f1_scores'])[:, -6:-1]), np.std(np.array(s['val_f1_scores'])[:, -6:-1])])\n",
    "    \n",
    "data = np.array(data)\n",
    "df = pd.DataFrame(data, columns=['vocab_sizes', 'mean f1 score', 'std'])\n",
    "df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = ClassifierBinary(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model_tmp.forward(tmp[1]['input'], tmp[1]['cl_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0936  0.9064\n",
       " 0.0510  0.9490\n",
       " 0.0435  0.9565\n",
       " 0.0318  0.9682\n",
       " 0.1032  0.8968\n",
       " 0.0978  0.9022\n",
       "[torch.cuda.FloatTensor of size 6x2 (GPU 0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.9064\n",
       "  0.9490\n",
       "  0.9565\n",
       "  0.9682\n",
       "  0.8968\n",
       "  0.9022\n",
       " [torch.cuda.FloatTensor of size 6 (GPU 0)], \n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " [torch.cuda.LongTensor of size 6 (GPU 0)])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 label_index,\n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=6, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        super(ClassifierBinary, self).__init__()\n",
    "        \"\"\"\n",
    "            A model from paper \"A Convolutional Attention Model for Text Classification\" \n",
    "            by Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He \n",
    "            http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf\n",
    "            With added character level features\n",
    "            \n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_of_cl_features = num_of_cl_features\n",
    "        \n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim).cuda()\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
    "                 # // 2 is because we would like to concat hidden states, \n",
    "                # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2).cuda()\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1).cuda()\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 2).cuda()\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2)\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1)\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 2)\n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if(self.use_cuda):\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda(), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq, cl_features=None):\n",
    "        embed = self.embeddings(input_seq)\n",
    "        output, _ = self.lstm(embed, self.hidden)\n",
    "        \n",
    "        conv_out = self.conv(embed.permute(0, 2, 1))\n",
    "        \n",
    "        attention_tensor = torch.mean(conv_out, dim=1)\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        \n",
    "        if(cl_features is not None and self.num_of_cl_features == cl_features.data.shape[1]):\n",
    "            features = torch.cat((features, cl_features), dim=1)\n",
    "        elif(cl_features is not None and self.num_of_cl_features != cl_features.data.shape[1]):\n",
    "            print(\"\"\"Recieved unexpected number of character level features. \n",
    "                     Model expected to recieve {} features, but received {}. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features, cl_features.data.shape[1]))\n",
    "            raise ValueError()\n",
    "        elif(cl_features is None and self.num_of_cl_features > 0):\n",
    "            print(\"\"\"Model expected to recieve {} features, but received None. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features))\n",
    "            raise ValueError()\n",
    "            \n",
    "        predictions = nn.functional.softmax(self.linear_final(features), dim=1)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def train (self, \n",
    "               n_train_batches=2000,\n",
    "               n_valid_batches=500,\n",
    "               lr = 1e-3,\n",
    "               weight_decay = 1e-5,\n",
    "               epochs = 15):\n",
    "        \n",
    "        dataset = ToxicTextsDatasetBinary(self.label_index, n_train_batches=n_train_batches, n_valid_batches=n_valid_batches),\n",
    "                   \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if(use_cuda): loss_function = nn.CrossEntropyLoss().cuda()\n",
    "        else: loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_stats = {'train_losses':[], 'valid_losses':[], val_f1_scores:[]}\n",
    "        \n",
    "        train_stats['train_losses'].append([0])\n",
    "        train_stats['valid_losses'].append([0])\n",
    "        train_stats['val_f1_scores'].append([0])\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            all_predictions = torch.zeros(1, 1)\n",
    "            all_true_labels = torch.zeros(1, 1)\n",
    "\n",
    "            for mode in ['train', 'valid']:\n",
    "                dataset.mode = mode\n",
    "                dataset.shuffle()\n",
    "                for sample in dataset:\n",
    "                    if(mode == 'train'):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if(Multiple_gpus):\n",
    "                            model.module.init_hidden()\n",
    "                        else:\n",
    "                            model.init_hidden()\n",
    "\n",
    "                        pred = model.forward(sample['input'], sample['cl_features'])\n",
    "\n",
    "                        loss = loss_function(pred, sample['labels'])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_stats[-1]['train_losses'][-1][-1] += loss.data[0]\n",
    "                    else:\n",
    "                        if(Multiple_gpus):\n",
    "                            model.module.init_hidden()\n",
    "                        else:\n",
    "                            model.init_hidden()\n",
    "\n",
    "                        pred = model.forward(sample['input'], sample['cl_features'])\n",
    "                        train_stats[-1]['valid_losses'][-1][-1] += loss_function(pred, sample['labels']).data[0]\n",
    "\n",
    "                        _, pred = torch.max(pred.data, 1)\n",
    "                        all_predictions = torch.cat((all_predictions, pred.cpu()))\n",
    "                        all_true_labels = torch.cat((all_true_labels, sample['labels'].data.cpu()))\n",
    "\n",
    "\n",
    "            all_predictions = all_predictions.numpy()\n",
    "            all_true_labels = all_true_labels.numpy()\n",
    "\n",
    "#             all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "\n",
    "            train_stats[-1]['val_f1_scores'][-1][-1] = f1_score(all_true_labels, all_predictions)\n",
    "            \n",
    "            \n",
    "#             print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; validation F1 score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "#                                                                              train_stats[-1]['train_losses'][-1][-1], \n",
    "#                                                                              train_stats[-1]['valid_losses'][-1][-1], \n",
    "#                                                                              train_stats[-1]['val_f1_scores'][-1][-1],\n",
    "#                                                                             (epochs - i)*(time.time() - start)/(i+1)))\n",
    "            train_stats[-1]['train_losses'][-1].append(0)\n",
    "            train_stats[-1]['valid_losses'][-1].append(0)\n",
    "            train_stats[-1]['val_f1_scores'][-1].append(0)\n",
    "        \n",
    "        return train_stats\n",
    "        \n",
    "class EnsembleBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=6, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        super(EnsembleBinary, self).__init__()\n",
    "        \n",
    "        self.classifier1 = ClassifierBinary(vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier2 = ClassifierBinary(vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier3 = ClassifierBinary(vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier4 = ClassifierBinary(vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier5 = ClassifierBinary(vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier6 = ClassifierBinary(vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        \n",
    "        def train (self, \n",
    "               n_train_batches=2000,\n",
    "               n_valid_batches=500,\n",
    "               lr = 1e-3,\n",
    "               weight_decay = 1e-5,\n",
    "               epochs = 15):\n",
    "            \n",
    "            train_stats1 = self.classifier1.train(n_train_batches, n_valid_batches, lr, weight_decay, epochs)\n",
    "            train_stats2 = self.classifier2.train(n_train_batches, n_valid_batches, lr, weight_decay, epochs)\n",
    "            train_stats3 = self.classifier3.train(n_train_batches, n_valid_batches, lr, weight_decay, epochs)\n",
    "            train_stats4 = self.classifier4.train(n_train_batches, n_valid_batches, lr, weight_decay, epochs)\n",
    "            train_stats5 = self.classifier5.train(n_train_batches, n_valid_batches, lr, weight_decay, epochs)\n",
    "            train_stats6 = self.classifier6.train(n_train_batches, n_valid_batches, lr, weight_decay, epochs)\n",
    "            \n",
    "            return train_stats1, train_stats2, train_stats3, train_stats4, train_stats5, train_stats6\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 10))\n",
    "# for i in range(9):\n",
    "#     x = np.arange(10)\n",
    "#     y = np.mean(np.array(train_stats[i]['val_f1_scores'])[:, :-1], axis=0)\n",
    "#     std = np.std(np.array(train_stats[i]['val_f1_scores'])[:, :-1], axis=0)\n",
    "        \n",
    "#     plt.errorbar(x, y, yerr=std, label='lr = {}, wd = {}'.format(train_stats[i]['lr'], train_stats[i]['weight_decay']))\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# fig = plt.figure(figsize=(9, 9))\n",
    "# ax = fig.gca(projection='3d')\n",
    "\n",
    "# ax.view_init(30, 90)\n",
    "\n",
    "# xx, yy = np.log(np.meshgrid(learning_rates, weight_decays))\n",
    "\n",
    "# tmp_lr = {0.01:0, 0.001:1, 0.0001:2}\n",
    "# tmp_wd = {0.001:0, 0.0001:1, 0.00001:2}\n",
    "\n",
    "# z = np.zeros((3, 3))\n",
    "# z_std = np.zeros((3, 3))\n",
    "# for s in train_stats:\n",
    "#     z[tmp_lr[s['lr']], tmp_wd[s['weight_decay']]] = np.mean(np.array(s['val_f1_scores'])[:, -6:-1])\n",
    "#     z_std[tmp_lr[s['lr']], tmp_wd[s['weight_decay']]] = np.std(np.array(s['val_f1_scores'])[:, -6:-1])\n",
    "\n",
    "# ax.plot_surface(xx, yy, z, cmap=plt.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.vstack((np.exp(xx).flatten(), np.exp(yy).flatten(), z.flatten(), z_std.flatten())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data, columns=['learning rate', 'weight decay', 'mean f1 score', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
