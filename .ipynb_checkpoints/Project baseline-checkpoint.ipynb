{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as td\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicTextsDataset(td.Dataset):\n",
    "    def __init__(self, data_path='train.csv', \n",
    "                       n_train_batches=16000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.2,\n",
    "                       valid_size=0.1,\n",
    "                       batch_size=10, \n",
    "                       vocab_size=2000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0,\n",
    "                       use_cuda = True):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation \n",
    "                                      from all data or from separated parts of data (a chance \n",
    "                                      of intersection between training, testing and validation \n",
    "                                      data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                        dataset for testing. Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                         dataset for validation. Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] \n",
    "                         most frequently encountered tokens, all others will be encoded as \n",
    "                         UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset \n",
    "                    will be returned sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDataset, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2:])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, \n",
    "                                                    test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for \n",
    "            each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers, \n",
    "                        cl_features - list of floats (character level features)\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        cl_features = self.ComputeCharacterLevelFeatures(text)\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes, cl_features\n",
    "    \n",
    "    def ComputeCharacterLevelFeatures(self, text):\n",
    "        \"\"\"This function computes a character level features \n",
    "           INPUT: text - a python string\n",
    "           OUTPUT: cl_features - a list of floats\n",
    "               \n",
    "               cl_features[0] - lenght of text\n",
    "               cl_features[1] - mean of lenghts of all tokens in text\n",
    "               cl_features[2] - ratio of capital letters in text\n",
    "               cl_features[3] - ratio of non-letter symbols in text\n",
    "        \"\"\"\n",
    "        text_len = float(len(text))\n",
    "        \n",
    "        cl_features = [\n",
    "            text_len,\n",
    "            np.mean([len(token) for token in self.Smart_Split(text)]),\n",
    "            len(re.findall(r'[A-Z]', text)) / text_len,\n",
    "            (1. - len(re.findall(r'[a-zA-Z]', text)) / text_len)\n",
    "        ]\n",
    "        \n",
    "        return cl_features\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with \n",
    "           corresponding labels). Produced dataset is drawn with repetition from initial data, \n",
    "           and therefore can contain duplicates Depending on n_batches, it will do either \n",
    "           undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text \n",
    "                     as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples \n",
    "                             in dataset is equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch \n",
    "                            of inputs and dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        masks = self.MakeMasks(y)\n",
    "        n_subbatches = n_batches // len(masks)\n",
    "        \n",
    "        if(self.verbose >= 2): print('\\n')\n",
    "        \n",
    "        for mask in masks:\n",
    "            if(self.verbose >= 2): print('\\tApplying mask: ' + mask['name'] + '... ', end='')\n",
    "            dataset += self.CreateDatasetFromXY(X[mask['mask']], y[mask['mask']], n_subbatches)\n",
    "            if(self.verbose >= 2): print('Completed')\n",
    "                \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples \n",
    "        with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                     text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number \n",
    "                             of samples in dataset is equal to n_batches * batch_size = \n",
    "                             len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th \n",
    "                            batch of inputs and dataset[i]['labels'] - corresponding \n",
    "                            batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and \n",
    "        # place texts of the same lenght in the same position in this dictionary. \n",
    "        # This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        for i in range(len(X)):\n",
    "            codes, cl_features = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, cl_features, y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, cl_features, y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more \n",
    "        # samples of lenght ~10-50 tokens and much smaller number of samples of lenght \n",
    "        # 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of \n",
    "            # lists and numpy array to torch tensor :)\n",
    "            if(self.use_cuda):\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False).cuda()}\n",
    "            else:\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False)}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                       text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                       classification labels (label != 0 is assumed to be \"interesting\" - \n",
    "                       we prioretize tokens encoundered in examples with at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[np.sum(y, axis=1) > 0] \n",
    "        X_relevant += shuffle(X[np.sum(y, axis=1) == 0])[:len(X_relevant)] \n",
    "        # we add random portion of \"all-negative\" data of equal size \n",
    "         \n",
    "        for text in X_relevant:\n",
    "            tokens = self.Smart_Split(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly \n",
    "        # classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the \n",
    "        # same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "        out = text.strip().lower().replace('\\n', ' ')\n",
    "        out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "        out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "        out = out.replace('.  .  .', '...')\n",
    "        return out.split()\n",
    "\n",
    "    def MakeMasks(self, y):\n",
    "        \"\"\"this function makes masks (bool np.arrays of length y). Each mask is \n",
    "        cunstructed so that X[mask] is a part of data grouped by some combination \n",
    "        of labels (for example - all data with al labels = 0, or all data with\n",
    "        first class label = 1 and all other equal to 0, or all data with all \n",
    "        labels equal to 1)\n",
    "            INPUT: y - np.array of shape [n_samples, n_classes]\n",
    "            OUTPUT: masks - list of bool np.arrays of length y\n",
    "        \"\"\"\n",
    "        \n",
    "        def not_i_col(y, i):\n",
    "            \"\"\"Utility function that returns all columns of y, except i-th\"\"\"\n",
    "            mask = np.array([True, True, True, True, True, True])\n",
    "            mask[i] = False\n",
    "            return y[:, mask]\n",
    "\n",
    "        # mask for data with label_excluded_i = 1 and all others = 0\n",
    "        # important: there is no data for label_1 = 1 and all others equal to 0, \n",
    "        # so skipping that mask\n",
    "        mask1 = []\n",
    "        for excluded_i in range(6):\n",
    "            mask1.append(np.logical_and(y[:, excluded_i] == 1, \n",
    "                                        np.sum(not_i_col(y, excluded_i), axis=1) == 0))\n",
    "\n",
    "        # masks for 2, 3, 4, 5 and 6 labels respectivly equal to 1 (here we do not care, \n",
    "        # which label (i.e. label_1, label_2, ...) \n",
    "        # is equal to 1, just that there is exactly n=2,3,.. labels equal to 1)\n",
    "        mask2 = np.sum(y, axis=1) == 2\n",
    "        mask3 = np.sum(y, axis=1) == 3\n",
    "        mask4 = np.sum(y, axis=1) == 4\n",
    "        mask5 = np.sum(y, axis=1) == 5\n",
    "        mask6 = np.sum(y, axis=1) == 6\n",
    "\n",
    "        mask0 = (np.sum(y, axis=1) == 0)\n",
    "\n",
    "        # let's save all masks in one list:\n",
    "        masks = [{'mask':mask0, 'name':'all-negative data'}, \n",
    "                 {'mask':mask1[0], 'name':'only fisrt class labeled positive'},\n",
    "                 {'mask':mask1[2], 'name':'only third class labeled positive'},\n",
    "                 {'mask':mask1[3], 'name':'only fourth class labeled positive'},\n",
    "                 {'mask':mask1[4], 'name':'only fifth class labeled positive'},\n",
    "                 {'mask':mask1[5], 'name':'only sixth class labeled positive'},\n",
    "                 {'mask':mask2, 'name':'exactly two positive labels'},\n",
    "                 {'mask':mask3, 'name':'exactly three positive labels'},\n",
    "                 {'mask':mask4, 'name':'exactly four positive labels'},\n",
    "                 {'mask':mask5, 'name':'exactly five positive labels'},\n",
    "                 {'mask':mask6, 'name':'all-positive data'}]\n",
    "            \n",
    "        if(self.verbose >= 2): print('\\n\\tMasks created (a reminder - no data for \"only second class labeled positive\")', end='')\n",
    "        \n",
    "        return masks\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicTextsDatasetBinary(td.Dataset):\n",
    "    def __init__(self, label_index,\n",
    "                       data_path='train.csv', \n",
    "                       n_train_batches=4000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.,\n",
    "                       valid_size=0.3,\n",
    "                       batch_size=6, \n",
    "                       vocab_size=2000,\n",
    "                       ngram_max_features=10000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0,\n",
    "                       use_cuda = True):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation \n",
    "                                      from all data or from separated parts of data (a chance \n",
    "                                      of intersection between training, testing and validation \n",
    "                                      data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                        dataset for testing. Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                         dataset for validation. Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] \n",
    "                         most frequently encountered tokens, all others will be encoded as \n",
    "                         UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset \n",
    "                    will be returned sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDatasetBinary, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.use_cuda = use_cuda\n",
    "        self.ngram_max_features = ngram_max_features\n",
    "        \n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2+label_index])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, \n",
    "                                                    test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            if(n_test_batches > 0): self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            else: self.test_dataset = []\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            if(n_valid_batches > 0): self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            else: self.valid_dataset = []\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for \n",
    "            each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers, \n",
    "                        cl_features - list of floats (character level features)\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        cl_features = self.ComputeCharacterLevelFeatures(text)\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes, cl_features\n",
    "    \n",
    "    def ComputeCharacterLevelFeatures(self, text):\n",
    "        \"\"\"This function computes a character level features \n",
    "           INPUT: text - a python string\n",
    "           OUTPUT: cl_features - a list of floats\n",
    "               \n",
    "               cl_features[0] - lenght of text\n",
    "               cl_features[1] - mean of lenghts of all tokens in text\n",
    "               cl_features[2] - ratio of capital letters in text\n",
    "               cl_features[3] - ratio of non-letter symbols in text\n",
    "        \"\"\"\n",
    "        text_len = float(len(text))\n",
    "        \n",
    "        cl_features = [\n",
    "            text_len,\n",
    "            np.mean([len(token) for token in self.Smart_Split(text)]),\n",
    "            len(re.findall(r'[A-Z]', text)) / text_len,\n",
    "            (1. - len(re.findall(r'[a-zA-Z]', text)) / text_len)\n",
    "        ]\n",
    "        \n",
    "        return cl_features\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with \n",
    "           corresponding labels). Produced dataset is drawn with repetition from initial data, \n",
    "           and therefore can contain duplicates Depending on n_batches, it will do either \n",
    "           undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text \n",
    "                     as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples \n",
    "                             in dataset is equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch \n",
    "                            of inputs and dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        n_subbatches = n_batches // 2\n",
    "        \n",
    "        mask = (y == 1)\n",
    "        sub_X, sub_y = shuffle(X[mask], y[mask])\n",
    "        dataset += self.CreateDatasetFromXY(sub_X[:self.batch_size*n_batches], \n",
    "                                            sub_y[:self.batch_size*n_batches], n_subbatches)\n",
    "        \n",
    "        mask = (y == 0)\n",
    "        sub_X, sub_y = shuffle(X[mask], y[mask])\n",
    "        dataset += self.CreateDatasetFromXY(sub_X[:self.batch_size*n_batches], \n",
    "                                            sub_y[:self.batch_size*n_batches], n_subbatches)\n",
    "        \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples \n",
    "        with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                     text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number \n",
    "                             of samples in dataset is equal to n_batches * batch_size = \n",
    "                             len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th \n",
    "                            batch of inputs and dataset[i]['labels'] - corresponding \n",
    "                            batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and \n",
    "        # place texts of the same lenght in the same position in this dictionary. \n",
    "        # This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        ngram_features = self.ngram_vectorizer.transform(X)\n",
    "        for i in range(len(X)):\n",
    "            codes, cl_features = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, cl_features, np.array(ngram_features[i].todense()).flatten(), y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, cl_features, np.array(ngram_features[i].todense()).flatten(), y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more \n",
    "        # samples of lenght ~10-50 tokens and much smaller number of samples of lenght \n",
    "        # 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of \n",
    "            # lists and numpy array to torch tensor :)\n",
    "            if(self.use_cuda):\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'labels':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 3].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'ngram_features':Variable(torch.FloatTensor(\n",
    "                         np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False).cuda()}\n",
    "            else:\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'labels':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 3].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'ngram_features':Variable(torch.FloatTensor(\n",
    "                         np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False)}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                       text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                       classification labels (label != 0 is assumed to be \"interesting\" - \n",
    "                       we prioretize tokens encoundered in examples with at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[y == 1] \n",
    "        X_relevant += shuffle(X[y == 0])[:len(X_relevant)] \n",
    "        # we add random portion of \"all-negative\" data of equal size \n",
    "           \n",
    "        self.ngram_vectorizer = TfidfVectorizer(analyzer='char', \n",
    "                                                ngram_range=(2, 5), \n",
    "                                                strip_accents = 'unicode',\n",
    "                                                max_features=self.ngram_max_features)\n",
    "        processed_x = np.array(pd.DataFrame(X_relevant)[0].apply(lambda x: self.prepare_for_char_n_gram(x)))\n",
    "        self.ngram_vectorizer.fit(processed_x)\n",
    "            \n",
    "        for text in processed_x:\n",
    "            tokens = text.split()\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly \n",
    "        # classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the \n",
    "        # same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "#         out = text.strip().lower().replace('\\n', ' ')\n",
    "#         out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "#         out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "#         out = out.replace('.  .  .', '...')\n",
    "        return self.prepare_for_char_n_gram(text).split()\n",
    "\n",
    "    def prepare_for_char_n_gram(self, text):\n",
    "        \"\"\" Simple text clean up process (written by olivier - https://www.kaggle.com/ogrellier) \"\"\"\n",
    "        # 1. Go to lower case (only good for english)\n",
    "        # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "        clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "        # 2. Drop \\n and  \\t\n",
    "        clean = clean.replace(b\"\\n\", b\" \")\n",
    "        clean = clean.replace(b\"\\t\", b\" \")\n",
    "        clean = clean.replace(b\"\\b\", b\" \")\n",
    "        clean = clean.replace(b\"\\r\", b\" \")\n",
    "        \n",
    "        # 4. Drop puntuation\n",
    "        # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "        exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "        clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "        # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "        clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "        # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "        clean = re.sub(b'\\s+', b' ', clean)\n",
    "        # Remove ending space if any\n",
    "        clean = re.sub(b'\\s+$', b'', clean)\n",
    "        # 7. Now replace words by words surrounded by # signs\n",
    "        # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "        # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "        clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "        clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "        return str(clean, 'utf-8')\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=20000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=64, \n",
    "                 batch_size=10,\n",
    "                 ngram_features=10000,\n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        \"\"\"\n",
    "            A model from paper \"A Convolutional Attention Model for Text Classification\" \n",
    "            by Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He \n",
    "            http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf\n",
    "            With modified outter layer (softmax -> sigmoid) for multilabel classification\n",
    "            and added character level features\n",
    "            \n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_of_cl_features = num_of_cl_features\n",
    "        self.ngram_features = ngram_features\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim).cuda()\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
    "                 # // 2 is because we would like to concat hidden states, \n",
    "                # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2).cuda()\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1).cuda()\n",
    "            self.linear_final = nn.Linear(2*hidden_dim + num_of_cl_features, 6).cuda()\n",
    "            self.linear_ngram = nn.Linear(ngram_features, hidden_dim)\n",
    "            # we have 6 classes to predict\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2)\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1)\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 6) # we have 6 classes to predict\n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "#         self.attention = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 0))\n",
    "#         )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if(self.use_cuda):\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda(), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq, cl_features=None):\n",
    "        embed = self.embeddings(input_seq)\n",
    "        output, _ = self.lstm(embed, self.hidden)\n",
    "        \n",
    "        conv_out = self.conv(embed.permute(0, 2, 1))\n",
    "        \n",
    "        attention_tensor = torch.mean(conv_out, dim=1)\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        \n",
    "        if(cl_features is not None and self.num_of_cl_features == cl_features.data.shape[1]):\n",
    "            features = torch.cat((features, cl_features), dim=1)\n",
    "        elif(cl_features is not None and self.num_of_cl_features != cl_features.data.shape[1]):\n",
    "            print(\"\"\"Recieved unexpected number of character level features. \n",
    "                     Model expected to recieve {} features, but received {}. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features, cl_features.data.shape[1]))\n",
    "            raise ValueError()\n",
    "        elif(cl_features is None and self.num_of_cl_features > 0):\n",
    "            print(\"\"\"Model expected to recieve {} features, but received None. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features))\n",
    "            raise ValueError()\n",
    "            \n",
    "        predictions = nn.functional.sigmoid(self.linear_final(features))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process (written by olivier - https://www.kaggle.com/ogrellier) \"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    \n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    \n",
    "    return str(clean, 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-3\n",
    "# weight_decay = 1e-5\n",
    "# cross_validation = 3\n",
    "\n",
    "# vocab_sizes = [3000, 5000, 7000]\n",
    "# embedding_dim = 200\n",
    "# hidden_dim = 100\n",
    "# conv_channels = 32\n",
    "\n",
    "# epochs = 15\n",
    "# batch_size = 6 # big batch sizes are not recomended, \n",
    "#                # since a lot of batches have 1 or 2 samples, repeated batch_size times.\n",
    "#                # for now a batch_size of 5 to 15 seems reasonable\n",
    "# use_cuda = True\n",
    "\n",
    "# train_stats = []\n",
    "\n",
    "# for vocab_size in tqdm.tqdm(vocab_sizes):\n",
    "#     train_stats.append({'vocab_sizes':vocab_size, 'train_losses':[], 'valid_losses':[], 'val_f1_scores':[]})\n",
    "    \n",
    "#     for _ in range(cross_validation):\n",
    "\n",
    "#         dataset = ToxicTextsDataset(n_train_batches=3000, \n",
    "#                                     n_test_batches=50, \n",
    "#                                     n_valid_batches=1000,\n",
    "#                                     valid_size=0.3,\n",
    "#                                     test_size=0.,\n",
    "#                                     batch_size=batch_size, \n",
    "#                                     vocab_size=vocab_size, \n",
    "#                                     verbose=0,\n",
    "#                                     use_cuda = use_cuda)\n",
    "\n",
    "#         Multiple_gpus = False\n",
    "\n",
    "#         model = LSTMClassifier(vocab_size=vocab_size, \n",
    "#                                embedding_dim = embedding_dim, \n",
    "#                                hidden_dim=hidden_dim, \n",
    "#                                conv_channels=conv_channels,\n",
    "#                                batch_size=batch_size, \n",
    "#                                use_cuda=use_cuda)\n",
    "\n",
    "#         # todo:\n",
    "\n",
    "#         # if (Multiple_gpus and torch.cuda.device_count() > 1):\n",
    "#         #     print(\"Detected {} gpu's. Using {} of them.\".format(torch.cuda.device_count(), torch.cuda.device_count()))\n",
    "#         #     model.num_gpus = torch.cuda.device_count()\n",
    "#         #     model = nn.DataParallel(model, dim=0)\n",
    "#         # else:\n",
    "#         #     Multiple_gpus = False\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "#         if(use_cuda): loss_function = nn.MultiLabelSoftMarginLoss().cuda()\n",
    "#         else: loss_function = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "#         train_stats[-1]['train_losses'].append([0])\n",
    "#         train_stats[-1]['valid_losses'].append([0])\n",
    "#         train_stats[-1]['val_f1_scores'].append([0])\n",
    "\n",
    "# #         print('=========================================')\n",
    "# #         print(\"Start of the training.\")\n",
    "#         start = time.time()\n",
    "\n",
    "#         for i in range(epochs):\n",
    "\n",
    "#             all_predictions = Variable(torch.zeros(1, 6))\n",
    "#             all_true_labels = Variable(torch.zeros(1, 6))\n",
    "\n",
    "#             for mode in ['train', 'valid']:\n",
    "#                 dataset.mode = mode\n",
    "#                 dataset.shuffle()\n",
    "#                 for sample in dataset:\n",
    "#                     if(mode == 'train'):\n",
    "#                         optimizer.zero_grad()\n",
    "\n",
    "#                         if(Multiple_gpus):\n",
    "#                             model.module.init_hidden()\n",
    "#                         else:\n",
    "#                             model.init_hidden()\n",
    "\n",
    "#                         pred = model.forward(sample['input'], sample['cl_features'])\n",
    "\n",
    "#                         loss = loss_function(pred, sample['labels'])\n",
    "\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "#                         train_stats[-1]['train_losses'][-1][-1] += loss.data[0]\n",
    "#                     else:\n",
    "#                         if(Multiple_gpus):\n",
    "#                             model.module.init_hidden()\n",
    "#                         else:\n",
    "#                             model.init_hidden()\n",
    "\n",
    "#                         pred = model.forward(sample['input'], sample['cl_features'])\n",
    "#                         train_stats[-1]['valid_losses'][-1][-1] += loss_function(pred, sample['labels']).data[0]\n",
    "\n",
    "#                         all_predictions = torch.cat((all_predictions, pred.cpu()))\n",
    "#                         all_true_labels = torch.cat((all_true_labels, sample['labels'].cpu()))\n",
    "\n",
    "\n",
    "#             all_predictions = all_predictions.data.numpy()\n",
    "#             all_true_labels = all_true_labels.data.numpy()\n",
    "\n",
    "#             all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "\n",
    "#             train_stats[-1]['val_f1_scores'][-1][-1] = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "            \n",
    "            \n",
    "# #             print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; validation F1 score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "# #                                                                              train_stats[-1]['train_losses'][-1][-1], \n",
    "# #                                                                              train_stats[-1]['valid_losses'][-1][-1], \n",
    "# #                                                                              train_stats[-1]['val_f1_scores'][-1][-1],\n",
    "# #                                                                             (epochs - i)*(time.time() - start)/(i+1)))\n",
    "#             train_stats[-1]['train_losses'][-1].append(0)\n",
    "#             train_stats[-1]['valid_losses'][-1].append(0)\n",
    "#             train_stats[-1]['val_f1_scores'][-1].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 label_index,\n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=6, \n",
    "                 conv_channels=32, \n",
    "                 ngram_features=10000,\n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        \n",
    "        super(ClassifierBinary, self).__init__()\n",
    "        \"\"\"\n",
    "            A model from paper \"A Convolutional Attention Model for Text Classification\" \n",
    "            by Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He \n",
    "            http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf\n",
    "            With added character level features\n",
    "            \n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_of_cl_features = num_of_cl_features\n",
    "        self.ngram_features = ngram_features\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim).cuda()\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
    "                 # // 2 is because we would like to concat hidden states, \n",
    "                # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2).cuda()\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1).cuda()\n",
    "            self.linear_final = nn.Linear(2*hidden_dim + num_of_cl_features, 2).cuda()\n",
    "            self.linear_ngram = nn.Linear(ngram_features, hidden_dim).cuda()\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2)\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1)\n",
    "            self.linear_final = nn.Linear(2*hidden_dim + num_of_cl_features, 2)\n",
    "            self.linear_ngram = nn.Linear(ngram_features, hidden_dim)\n",
    "            \n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if(self.use_cuda):\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda(), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq, ngram_features, cl_features=None, batched=True):\n",
    "        \n",
    "        embed = self.embeddings(input_seq)\n",
    "            \n",
    "        if(batched):\n",
    "            output, _ = self.lstm(embed, self.hidden)\n",
    "        else:\n",
    "            output, _ = self.lstm(torch.cat((embed, )*self.batch_size), self.hidden)\n",
    "            output = output[:1, :, :]\n",
    "            \n",
    "        conv_out = self.conv(embed.permute(0, 2, 1))\n",
    "        \n",
    "        attention_tensor = nn.functional.sigmoid(torch.mean(conv_out, dim=1))\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        ngram_compressed = self.linear_ngram(ngram_features)\n",
    "        \n",
    "        if(cl_features is not None and self.num_of_cl_features == cl_features.data.shape[1]):\n",
    "            features = torch.cat((features, cl_features, ngram_compressed), dim=1)\n",
    "        elif(cl_features is not None and self.num_of_cl_features != cl_features.data.shape[1]):\n",
    "            print(\"\"\"Recieved unexpected number of character level features. \n",
    "                     Model expected to recieve {} features, but received {}. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features, cl_features.data.shape[1]))\n",
    "            raise ValueError()\n",
    "        elif(cl_features is None and self.num_of_cl_features > 0):\n",
    "            print(\"\"\"Model expected to recieve {} features, but received None. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features))\n",
    "            raise ValueError()\n",
    "            \n",
    "        predictions = nn.functional.softmax(self.linear_final(features), dim=1)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def train_ (self, \n",
    "               n_train_batches=2000,\n",
    "               n_valid_batches=500,\n",
    "               lr = 1e-3,\n",
    "               weight_decay = 1e-5,\n",
    "               epochs = 15,\n",
    "               separate_test_and_valid=False):\n",
    "        \n",
    "        self.dataset = ToxicTextsDatasetBinary(self.label_index,\n",
    "                                               vocab_size=self.vocab_size,\n",
    "                                               batch_size=self.batch_size,\n",
    "                                          n_train_batches=n_train_batches, \n",
    "                                          n_valid_batches=n_valid_batches,\n",
    "                                              ngram_max_features=self.ngram_features, \n",
    "                                               use_cuda=self.use_cuda,\n",
    "                                              separate_test_and_valid=separate_test_and_valid)\n",
    "                   \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if(self.use_cuda): loss_function = nn.CrossEntropyLoss().cuda()\n",
    "        else: loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#         train_stats = {'train_losses':[], 'valid_losses':[], 'val_f1_scores':[]}\n",
    "        \n",
    "#         train_stats['train_losses'].append(0)\n",
    "#         train_stats['valid_losses'].append(0)\n",
    "#         train_stats['val_f1_scores'].append(0)\n",
    "\n",
    "#         start = time.time()\n",
    "        print('====================================================')\n",
    "        print('Start of the training for {} label'.format(self.label_index + 1))\n",
    "        for i in range(epochs):\n",
    "\n",
    "#             all_predictions = torch.zeros(1, 1)\n",
    "#             all_true_labels = torch.zeros(1, 1)\n",
    "\n",
    "#             for mode in ['train', 'valid']:\n",
    "#                 self.dataset.mode = mode\n",
    "            self.dataset.shuffle()\n",
    "            for sample in self.dataset:\n",
    "#                     if(mode == 'train'):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                self.init_hidden()\n",
    "\n",
    "                pred = self.forward(sample['input'], sample['ngram_features'], sample['cl_features'])\n",
    "\n",
    "                loss = loss_function(pred, sample['labels'])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#                 train_stats['train_losses'][-1] += loss.data[0]\n",
    "#                     else:\n",
    "#                         self.init_hidden()\n",
    "\n",
    "#                         pred = self.forward(sample['input'], sample['ngram_features'], sample['cl_features'])\n",
    "#                         train_stats['valid_losses'][-1] += loss_function(pred, sample['labels']).data[0]\n",
    "\n",
    "#                         pred = pred.data[:, 1]\n",
    "#                         all_predictions = torch.cat((all_predictions, torch.FloatTensor(pred.cpu().numpy())))\n",
    "#                         all_true_labels = torch.cat((all_true_labels, torch.FloatTensor(\n",
    "#                                                                         sample['labels'].data.cpu().numpy())))\n",
    "\n",
    "\n",
    "#             all_predictions = all_predictions.numpy()\n",
    "#             all_true_labels = all_true_labels.numpy()\n",
    "\n",
    "#             all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "\n",
    "#             train_stats['val_f1_scores'][-1] = roc_auc_score(all_true_labels, all_predictions)\n",
    "            \n",
    "            \n",
    "#             print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; roc auc score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "#                                                                              train_stats['train_losses'][-1], \n",
    "#                                                                              train_stats['valid_losses'][-1], \n",
    "#                                                                              train_stats['val_f1_scores'][-1],\n",
    "#                                                                             (epochs - i)*(time.time() - start)/(i+1)))\n",
    "#             train_stats['train_losses'].append(0)\n",
    "#             train_stats['valid_losses'].append(0)\n",
    "#             train_stats['val_f1_scores'].append(0)\n",
    "        \n",
    "#         del train_stats\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        codes, cl_features = self.dataset.encode(X)\n",
    "        ngrams = self.dataset.ngram_vectorizer.transform([X])\n",
    "        \n",
    "        Input = Variable(torch.LongTensor(np.array(codes)), \n",
    "                    requires_grad=False).resize(1, len(codes)).cuda()\n",
    "        \n",
    "        cl_features = Variable(torch.FloatTensor(np.array(cl_features)), \n",
    "                    requires_grad=False).resize(1, len(cl_features)).cuda()\n",
    "        \n",
    "        ngrams = Variable(torch.FloatTensor(np.array(ngrams.todense())), \n",
    "                    requires_grad=False).cuda()\n",
    "        \n",
    "        pred = self.forward(Input, ngrams, cl_features, batched=False)\n",
    "                       \n",
    "#         _, pred = torch.max(pred.data, 1)\n",
    "        \n",
    "        return pred.data[:, 1]\n",
    "                               \n",
    "class EnsembleBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=3, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 ngram_features=10000,\n",
    "                 num_of_cl_features=4):\n",
    "        super(EnsembleBinary, self).__init__()\n",
    "        \n",
    "        self.classifier1 = ClassifierBinary(0,\n",
    "                 vocab_size=vocab_size, \n",
    "                 embedding_dim = embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 batch_size=batch_size, \n",
    "                 conv_channels=conv_channels, \n",
    "                 ngram_features=ngram_features,\n",
    "                 use_cuda=use_cuda,\n",
    "                 num_of_cl_features=num_of_cl_features)\n",
    "        self.classifier2 = ClassifierBinary(1,\n",
    "                 vocab_size=vocab_size, \n",
    "                 embedding_dim = embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 batch_size=batch_size, \n",
    "                 conv_channels=conv_channels, \n",
    "                 ngram_features=ngram_features,\n",
    "                 use_cuda=use_cuda,\n",
    "                 num_of_cl_features=num_of_cl_features)\n",
    "        self.classifier3 = ClassifierBinary(2,\n",
    "                 vocab_size=vocab_size, \n",
    "                 embedding_dim = embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 batch_size=batch_size, \n",
    "                 conv_channels=conv_channels, \n",
    "                 ngram_features=ngram_features,\n",
    "                 use_cuda=use_cuda,\n",
    "                 num_of_cl_features=num_of_cl_features)\n",
    "        self.classifier4 = ClassifierBinary(3,\n",
    "                 vocab_size=vocab_size, \n",
    "                 embedding_dim = embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 batch_size=batch_size, \n",
    "                 conv_channels=conv_channels, \n",
    "                 ngram_features=ngram_features,\n",
    "                 use_cuda=use_cuda,\n",
    "                 num_of_cl_features=num_of_cl_features)\n",
    "        self.classifier5 = ClassifierBinary(4,\n",
    "                 vocab_size=vocab_size, \n",
    "                 embedding_dim = embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 batch_size=batch_size, \n",
    "                 conv_channels=conv_channels, \n",
    "                 ngram_features=ngram_features,\n",
    "                 use_cuda=use_cuda,\n",
    "                 num_of_cl_features=num_of_cl_features)\n",
    "        self.classifier6 = ClassifierBinary(5,\n",
    "                 vocab_size=vocab_size, \n",
    "                 embedding_dim = embedding_dim, \n",
    "                 hidden_dim=hidden_dim, \n",
    "                 batch_size=batch_size, \n",
    "                 conv_channels=conv_channels, \n",
    "                 ngram_features=ngram_features,\n",
    "                 use_cuda=use_cuda,\n",
    "                 num_of_cl_features=num_of_cl_features)\n",
    "        \n",
    "    def train_(self, \n",
    "           n_train_batches=1000,\n",
    "           n_valid_batches=0,\n",
    "           lr = 1e-4,\n",
    "           weight_decay = 1e-4,\n",
    "           epochs = 10,\n",
    "           separate_test_and_valid=False):\n",
    "\n",
    "        self.classifier1.train_(n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\n",
    "        del self.classifier1.dataset.train_dataset\n",
    "        del self.classifier1.dataset.valid_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "        self.classifier2.train_(n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\n",
    "        del self.classifier2.dataset.train_dataset\n",
    "        del self.classifier2.dataset.valid_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "        self.classifier3.train_(n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\n",
    "        del self.classifier3.dataset.train_dataset\n",
    "        del self.classifier3.dataset.valid_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "        self.classifier4.train_(n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\n",
    "        del self.classifier4.dataset.train_dataset\n",
    "        del self.classifier4.dataset.valid_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "        self.classifier5.train_(n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\n",
    "        del self.classifier5.dataset.train_dataset\n",
    "        del self.classifier5.dataset.valid_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "        self.classifier6.train_(n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\n",
    "        del self.classifier6.dataset.train_dataset\n",
    "        del self.classifier6.dataset.valid_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        return torch.stack((self.classifier1.predict(X), \n",
    "                            self.classifier2.predict(X),\n",
    "                            self.classifier3.predict(X),\n",
    "                            self.classifier4.predict(X), \n",
    "                            self.classifier5.predict(X),\n",
    "                            self.classifier6.predict(X))).transpose(0, 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble():\n",
    "    def __init__(self, n_classifiers_per_label=10):\n",
    "        \n",
    "#         self.classifiers = []\n",
    "        \n",
    "        df = pd.read_csv('test.csv')\n",
    "        df_pred = pd.read_csv('sample_submission.csv')\n",
    "        df_pred = df_pred.set_index(['id'])   \n",
    "        \n",
    "        for i in tqdm.tqdm_notebook(range(n_classifiers_per_label)):\n",
    "            E = EnsembleBinary()\n",
    "            E.train_()\n",
    "            if(i > 0):\n",
    "                df_pred = pd.read_csv('test_predictions.csv')\n",
    "                df_pred = df_pred.set_index(['id'])\n",
    "            \n",
    "            for ID, line in tqdm.tqdm_notebook(zip(df_pred.index, df.iloc[:, 0]), total=len(df)):\n",
    "                if(line != '\\u2003'):\n",
    "                    df_pred.loc[ID] = (df_pred.loc[ID]*(i+1.) + E.predict(line)[0])/(i+2.)\n",
    "                else:\n",
    "                    df_pred.loc[ID] = [0., 0., 0., 0., 0., 0.]\n",
    "            \n",
    "            del E\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            df_pred.to_csv('test_predictions.csv')\n",
    "#             self.classifiers[-1].train_()\n",
    "            #print('trained {}-th ensemble of binary classifiers!'.format(i))\n",
    "\n",
    "#             del self.classifiers[-1].classifier1.dataset.train_dataset\n",
    "#             del self.classifiers[-1].classifier2.dataset.train_dataset\n",
    "#             del self.classifiers[-1].classifier3.dataset.train_dataset\n",
    "#             del self.classifiers[-1].classifier4.dataset.train_dataset\n",
    "#             del self.classifiers[-1].classifier5.dataset.train_dataset\n",
    "#             del self.classifiers[-1].classifier6.dataset.train_dataset\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for classifier in self.classifiers:\n",
    "            predictions.append(classifier.predict(X).cpu().numpy())\n",
    "        \n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6636343e3c3b4f17962a6615bf6b24a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Start of the training for 1 label\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-17c734399367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classifiers_per_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-cca37e27bcd3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_classifiers_per_label)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classifiers_per_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsembleBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mdf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d8aa171af5d6>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\u001b[0m\n\u001b[1;32m    290\u001b[0m            separate_test_and_valid=False):\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparate_test_and_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d8aa171af5d6>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, n_train_batches, n_valid_batches, lr, weight_decay, epochs, separate_test_and_valid)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m#                 train_stats['train_losses'][-1] += loss.data[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Ensemble(n_classifiers_per_label=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of sample_submision influence\n",
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "df_pred = pd.read_csv('test_predictions.csv')\n",
    "df_pred = df_pred.set_index(['id'])\n",
    "\n",
    "for ID, line in tqdm.tqdm_notebook(zip(df_pred.index, df.iloc[:, 0]), total=len(df)):\n",
    "    if(line != '\\u2003'):\n",
    "        df_pred.loc[ID] = (df_pred.loc[ID]*13 - 0.5)/12.\n",
    "    else:\n",
    "        df_pred.loc[ID] = [0., 0., 0., 0., 0., 0.]\n",
    "\n",
    "df_pred.to_csv('test_predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 10))\n",
    "# for i in range(9):\n",
    "#     x = np.arange(10)\n",
    "#     y = np.mean(np.array(train_stats[i]['val_f1_scores'])[:, :-1], axis=0)\n",
    "#     std = np.std(np.array(train_stats[i]['val_f1_scores'])[:, :-1], axis=0)\n",
    "        \n",
    "#     plt.errorbar(x, y, yerr=std, label='lr = {}, wd = {}'.format(train_stats[i]['lr'], train_stats[i]['weight_decay']))\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# fig = plt.figure(figsize=(9, 9))\n",
    "# ax = fig.gca(projection='3d')\n",
    "\n",
    "# ax.view_init(30, 90)\n",
    "\n",
    "# xx, yy = np.log(np.meshgrid(learning_rates, weight_decays))\n",
    "\n",
    "# tmp_lr = {0.01:0, 0.001:1, 0.0001:2}\n",
    "# tmp_wd = {0.001:0, 0.0001:1, 0.00001:2}\n",
    "\n",
    "# z = np.zeros((3, 3))\n",
    "# z_std = np.zeros((3, 3))\n",
    "# for s in train_stats:\n",
    "#     z[tmp_lr[s['lr']], tmp_wd[s['weight_decay']]] = np.mean(np.array(s['val_f1_scores'])[:, -6:-1])\n",
    "#     z_std[tmp_lr[s['lr']], tmp_wd[s['weight_decay']]] = np.std(np.array(s['val_f1_scores'])[:, -6:-1])\n",
    "\n",
    "# ax.plot_surface(xx, yy, z, cmap=plt.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.vstack((np.exp(xx).flatten(), np.exp(yy).flatten(), z.flatten(), z_std.flatten())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data, columns=['learning rate', 'weight decay', 'mean f1 score', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
