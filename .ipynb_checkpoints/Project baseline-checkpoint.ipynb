{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as td\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicTextsDataset(td.Dataset):\n",
    "    def __init__(self, data_path='train.csv', \n",
    "                       n_train_batches=16000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.2,\n",
    "                       valid_size=0.1,\n",
    "                       batch_size=10, \n",
    "                       vocab_size=2000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0,\n",
    "                       use_cuda = True):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation \n",
    "                                      from all data or from separated parts of data (a chance \n",
    "                                      of intersection between training, testing and validation \n",
    "                                      data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                        dataset for testing. Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                         dataset for validation. Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] \n",
    "                         most frequently encountered tokens, all others will be encoded as \n",
    "                         UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset \n",
    "                    will be returned sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDataset, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2:])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, \n",
    "                                                    test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for \n",
    "            each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers, \n",
    "                        cl_features - list of floats (character level features)\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        cl_features = self.ComputeCharacterLevelFeatures(text)\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes, cl_features\n",
    "    \n",
    "    def ComputeCharacterLevelFeatures(self, text):\n",
    "        \"\"\"This function computes a character level features \n",
    "           INPUT: text - a python string\n",
    "           OUTPUT: cl_features - a list of floats\n",
    "               \n",
    "               cl_features[0] - lenght of text\n",
    "               cl_features[1] - mean of lenghts of all tokens in text\n",
    "               cl_features[2] - ratio of capital letters in text\n",
    "               cl_features[3] - ratio of non-letter symbols in text\n",
    "        \"\"\"\n",
    "        text_len = float(len(text))\n",
    "        \n",
    "        cl_features = [\n",
    "            text_len,\n",
    "            np.mean([len(token) for token in self.Smart_Split(text)]),\n",
    "            len(re.findall(r'[A-Z]', text)) / text_len,\n",
    "            (1. - len(re.findall(r'[a-zA-Z]', text)) / text_len)\n",
    "        ]\n",
    "        \n",
    "        return cl_features\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with \n",
    "           corresponding labels). Produced dataset is drawn with repetition from initial data, \n",
    "           and therefore can contain duplicates Depending on n_batches, it will do either \n",
    "           undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text \n",
    "                     as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples \n",
    "                             in dataset is equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch \n",
    "                            of inputs and dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        masks = self.MakeMasks(y)\n",
    "        n_subbatches = n_batches // len(masks)\n",
    "        \n",
    "        if(self.verbose >= 2): print('\\n')\n",
    "        \n",
    "        for mask in masks:\n",
    "            if(self.verbose >= 2): print('\\tApplying mask: ' + mask['name'] + '... ', end='')\n",
    "            dataset += self.CreateDatasetFromXY(X[mask['mask']], y[mask['mask']], n_subbatches)\n",
    "            if(self.verbose >= 2): print('Completed')\n",
    "                \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples \n",
    "        with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                     text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number \n",
    "                             of samples in dataset is equal to n_batches * batch_size = \n",
    "                             len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th \n",
    "                            batch of inputs and dataset[i]['labels'] - corresponding \n",
    "                            batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and \n",
    "        # place texts of the same lenght in the same position in this dictionary. \n",
    "        # This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        for i in range(len(X)):\n",
    "            codes, cl_features = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, cl_features, y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, cl_features, y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more \n",
    "        # samples of lenght ~10-50 tokens and much smaller number of samples of lenght \n",
    "        # 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of \n",
    "            # lists and numpy array to torch tensor :)\n",
    "            if(self.use_cuda):\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False).cuda()}\n",
    "            else:\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'labels':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False)}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                       text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                       classification labels (label != 0 is assumed to be \"interesting\" - \n",
    "                       we prioretize tokens encoundered in examples with at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[np.sum(y, axis=1) > 0] \n",
    "        X_relevant += shuffle(X[np.sum(y, axis=1) == 0])[:len(X_relevant)] \n",
    "        # we add random portion of \"all-negative\" data of equal size \n",
    "         \n",
    "        for text in X_relevant:\n",
    "            tokens = self.Smart_Split(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly \n",
    "        # classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the \n",
    "        # same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "        out = text.strip().lower().replace('\\n', ' ')\n",
    "        out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "        out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "        out = out.replace('.  .  .', '...')\n",
    "        return out.split()\n",
    "\n",
    "    def MakeMasks(self, y):\n",
    "        \"\"\"this function makes masks (bool np.arrays of length y). Each mask is \n",
    "        cunstructed so that X[mask] is a part of data grouped by some combination \n",
    "        of labels (for example - all data with al labels = 0, or all data with\n",
    "        first class label = 1 and all other equal to 0, or all data with all \n",
    "        labels equal to 1)\n",
    "            INPUT: y - np.array of shape [n_samples, n_classes]\n",
    "            OUTPUT: masks - list of bool np.arrays of length y\n",
    "        \"\"\"\n",
    "        \n",
    "        def not_i_col(y, i):\n",
    "            \"\"\"Utility function that returns all columns of y, except i-th\"\"\"\n",
    "            mask = np.array([True, True, True, True, True, True])\n",
    "            mask[i] = False\n",
    "            return y[:, mask]\n",
    "\n",
    "        # mask for data with label_excluded_i = 1 and all others = 0\n",
    "        # important: there is no data for label_1 = 1 and all others equal to 0, \n",
    "        # so skipping that mask\n",
    "        mask1 = []\n",
    "        for excluded_i in range(6):\n",
    "            mask1.append(np.logical_and(y[:, excluded_i] == 1, \n",
    "                                        np.sum(not_i_col(y, excluded_i), axis=1) == 0))\n",
    "\n",
    "        # masks for 2, 3, 4, 5 and 6 labels respectivly equal to 1 (here we do not care, \n",
    "        # which label (i.e. label_1, label_2, ...) \n",
    "        # is equal to 1, just that there is exactly n=2,3,.. labels equal to 1)\n",
    "        mask2 = np.sum(y, axis=1) == 2\n",
    "        mask3 = np.sum(y, axis=1) == 3\n",
    "        mask4 = np.sum(y, axis=1) == 4\n",
    "        mask5 = np.sum(y, axis=1) == 5\n",
    "        mask6 = np.sum(y, axis=1) == 6\n",
    "\n",
    "        mask0 = (np.sum(y, axis=1) == 0)\n",
    "\n",
    "        # let's save all masks in one list:\n",
    "        masks = [{'mask':mask0, 'name':'all-negative data'}, \n",
    "                 {'mask':mask1[0], 'name':'only fisrt class labeled positive'},\n",
    "                 {'mask':mask1[2], 'name':'only third class labeled positive'},\n",
    "                 {'mask':mask1[3], 'name':'only fourth class labeled positive'},\n",
    "                 {'mask':mask1[4], 'name':'only fifth class labeled positive'},\n",
    "                 {'mask':mask1[5], 'name':'only sixth class labeled positive'},\n",
    "                 {'mask':mask2, 'name':'exactly two positive labels'},\n",
    "                 {'mask':mask3, 'name':'exactly three positive labels'},\n",
    "                 {'mask':mask4, 'name':'exactly four positive labels'},\n",
    "                 {'mask':mask5, 'name':'exactly five positive labels'},\n",
    "                 {'mask':mask6, 'name':'all-positive data'}]\n",
    "            \n",
    "        if(self.verbose >= 2): print('\\n\\tMasks created (a reminder - no data for \"only second class labeled positive\")', end='')\n",
    "        \n",
    "        return masks\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicTextsDatasetBinary(td.Dataset):\n",
    "    def __init__(self, label_index,\n",
    "                       data_path='train.csv', \n",
    "                       n_train_batches=4000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.,\n",
    "                       valid_size=0.3,\n",
    "                       batch_size=6, \n",
    "                       vocab_size=2000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0,\n",
    "                       use_cuda = True):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation \n",
    "                                      from all data or from separated parts of data (a chance \n",
    "                                      of intersection between training, testing and validation \n",
    "                                      data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                        dataset for testing. Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                         dataset for validation. Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] \n",
    "                         most frequently encountered tokens, all others will be encoded as \n",
    "                         UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset \n",
    "                    will be returned sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDatasetBinary, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2+label_index])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, \n",
    "                                                    test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for \n",
    "            each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers, \n",
    "                        cl_features - list of floats (character level features)\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        cl_features = self.ComputeCharacterLevelFeatures(text)\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes, cl_features\n",
    "    \n",
    "    def ComputeCharacterLevelFeatures(self, text):\n",
    "        \"\"\"This function computes a character level features \n",
    "           INPUT: text - a python string\n",
    "           OUTPUT: cl_features - a list of floats\n",
    "               \n",
    "               cl_features[0] - lenght of text\n",
    "               cl_features[1] - mean of lenghts of all tokens in text\n",
    "               cl_features[2] - ratio of capital letters in text\n",
    "               cl_features[3] - ratio of non-letter symbols in text\n",
    "        \"\"\"\n",
    "        text_len = float(len(text))\n",
    "        \n",
    "        cl_features = [\n",
    "            text_len,\n",
    "            np.mean([len(token) for token in self.Smart_Split(text)]),\n",
    "            len(re.findall(r'[A-Z]', text)) / text_len,\n",
    "            (1. - len(re.findall(r'[a-zA-Z]', text)) / text_len)\n",
    "        ]\n",
    "        \n",
    "        return cl_features\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with \n",
    "           corresponding labels). Produced dataset is drawn with repetition from initial data, \n",
    "           and therefore can contain duplicates Depending on n_batches, it will do either \n",
    "           undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text \n",
    "                     as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples \n",
    "                             in dataset is equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch \n",
    "                            of inputs and dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        n_subbatches = n_batches // 2\n",
    "        \n",
    "        mask = (y == 1)\n",
    "        dataset += self.CreateDatasetFromXY(X[mask], y[mask], n_subbatches)\n",
    "        \n",
    "        mask = (y == 0)\n",
    "        dataset += self.CreateDatasetFromXY(X[mask], y[mask], n_subbatches)\n",
    "        \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples \n",
    "        with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                     text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number \n",
    "                             of samples in dataset is equal to n_batches * batch_size = \n",
    "                             len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th \n",
    "                            batch of inputs and dataset[i]['labels'] - corresponding \n",
    "                            batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and \n",
    "        # place texts of the same lenght in the same position in this dictionary. \n",
    "        # This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        for i in range(len(X)):\n",
    "            codes, cl_features = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, cl_features, y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, cl_features, y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more \n",
    "        # samples of lenght ~10-50 tokens and much smaller number of samples of lenght \n",
    "        # 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of \n",
    "            # lists and numpy array to torch tensor :)\n",
    "            if(self.use_cuda):\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'labels':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False).cuda()}\n",
    "            else:\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'labels':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False)}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                       text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                       classification labels (label != 0 is assumed to be \"interesting\" - \n",
    "                       we prioretize tokens encoundered in examples with at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[y == 1] \n",
    "        X_relevant += shuffle(X[y == 0])[:len(X_relevant)] \n",
    "        # we add random portion of \"all-negative\" data of equal size \n",
    "         \n",
    "        for text in X_relevant:\n",
    "            tokens = self.Smart_Split(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly \n",
    "        # classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the \n",
    "        # same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "        out = text.strip().lower().replace('\\n', ' ')\n",
    "        out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "        out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "        out = out.replace('.  .  .', '...')\n",
    "        return out.split()\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=20000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=64, \n",
    "                 batch_size=10, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        \"\"\"\n",
    "            A model from paper \"A Convolutional Attention Model for Text Classification\" \n",
    "            by Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He \n",
    "            http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf\n",
    "            With modified outter layer (softmax -> sigmoid) for multilabel classification\n",
    "            and added character level features\n",
    "            \n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_of_cl_features = num_of_cl_features\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim).cuda()\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
    "                 # // 2 is because we would like to concat hidden states, \n",
    "                # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2).cuda()\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1).cuda()\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 6).cuda() \n",
    "            # we have 6 classes to predict\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2)\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1)\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 6) # we have 6 classes to predict\n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "#         self.attention = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 2), stride=(1, 2), padding=(1, 0)),\n",
    "#             nn.Conv2d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=(3, 6), stride=1, padding=(1, 0)),\n",
    "#             nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 0))\n",
    "#         )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if(self.use_cuda):\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda(), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq, cl_features=None):\n",
    "        embed = self.embeddings(input_seq)\n",
    "        output, _ = self.lstm(embed, self.hidden)\n",
    "        \n",
    "        conv_out = self.conv(embed.permute(0, 2, 1))\n",
    "        \n",
    "        attention_tensor = torch.mean(conv_out, dim=1)\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        \n",
    "        if(cl_features is not None and self.num_of_cl_features == cl_features.data.shape[1]):\n",
    "            features = torch.cat((features, cl_features), dim=1)\n",
    "        elif(cl_features is not None and self.num_of_cl_features != cl_features.data.shape[1]):\n",
    "            print(\"\"\"Recieved unexpected number of character level features. \n",
    "                     Model expected to recieve {} features, but received {}. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features, cl_features.data.shape[1]))\n",
    "            raise ValueError()\n",
    "        elif(cl_features is None and self.num_of_cl_features > 0):\n",
    "            print(\"\"\"Model expected to recieve {} features, but received None. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features))\n",
    "            raise ValueError()\n",
    "            \n",
    "        predictions = nn.functional.sigmoid(self.linear_final(features))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "cross_validation = 3\n",
    "\n",
    "vocab_sizes = [3000, 5000, 7000]\n",
    "embedding_dim = 200\n",
    "hidden_dim = 100\n",
    "conv_channels = 32\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 6 # big batch sizes are not recomended, \n",
    "               # since a lot of batches have 1 or 2 samples, repeated batch_size times.\n",
    "               # for now a batch_size of 5 to 15 seems reasonable\n",
    "use_cuda = True\n",
    "\n",
    "train_stats = []\n",
    "\n",
    "for vocab_size in tqdm.tqdm(vocab_sizes):\n",
    "    train_stats.append({'vocab_sizes':vocab_size, 'train_losses':[], 'valid_losses':[], 'val_f1_scores':[]})\n",
    "    \n",
    "    for _ in range(cross_validation):\n",
    "\n",
    "        dataset = ToxicTextsDataset(n_train_batches=3000, \n",
    "                                    n_test_batches=50, \n",
    "                                    n_valid_batches=1000,\n",
    "                                    valid_size=0.3,\n",
    "                                    test_size=0.,\n",
    "                                    batch_size=batch_size, \n",
    "                                    vocab_size=vocab_size, \n",
    "                                    verbose=0,\n",
    "                                    use_cuda = use_cuda)\n",
    "\n",
    "        Multiple_gpus = False\n",
    "\n",
    "        model = LSTMClassifier(vocab_size=vocab_size, \n",
    "                               embedding_dim = embedding_dim, \n",
    "                               hidden_dim=hidden_dim, \n",
    "                               conv_channels=conv_channels,\n",
    "                               batch_size=batch_size, \n",
    "                               use_cuda=use_cuda)\n",
    "\n",
    "        # todo:\n",
    "\n",
    "        # if (Multiple_gpus and torch.cuda.device_count() > 1):\n",
    "        #     print(\"Detected {} gpu's. Using {} of them.\".format(torch.cuda.device_count(), torch.cuda.device_count()))\n",
    "        #     model.num_gpus = torch.cuda.device_count()\n",
    "        #     model = nn.DataParallel(model, dim=0)\n",
    "        # else:\n",
    "        #     Multiple_gpus = False\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if(use_cuda): loss_function = nn.MultiLabelSoftMarginLoss().cuda()\n",
    "        else: loss_function = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "        train_stats[-1]['train_losses'].append([0])\n",
    "        train_stats[-1]['valid_losses'].append([0])\n",
    "        train_stats[-1]['val_f1_scores'].append([0])\n",
    "\n",
    "#         print('=========================================')\n",
    "#         print(\"Start of the training.\")\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            all_predictions = Variable(torch.zeros(1, 6))\n",
    "            all_true_labels = Variable(torch.zeros(1, 6))\n",
    "\n",
    "            for mode in ['train', 'valid']:\n",
    "                dataset.mode = mode\n",
    "                dataset.shuffle()\n",
    "                for sample in dataset:\n",
    "                    if(mode == 'train'):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if(Multiple_gpus):\n",
    "                            model.module.init_hidden()\n",
    "                        else:\n",
    "                            model.init_hidden()\n",
    "\n",
    "                        pred = model.forward(sample['input'], sample['cl_features'])\n",
    "\n",
    "                        loss = loss_function(pred, sample['labels'])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_stats[-1]['train_losses'][-1][-1] += loss.data[0]\n",
    "                    else:\n",
    "                        if(Multiple_gpus):\n",
    "                            model.module.init_hidden()\n",
    "                        else:\n",
    "                            model.init_hidden()\n",
    "\n",
    "                        pred = model.forward(sample['input'], sample['cl_features'])\n",
    "                        train_stats[-1]['valid_losses'][-1][-1] += loss_function(pred, sample['labels']).data[0]\n",
    "\n",
    "                        all_predictions = torch.cat((all_predictions, pred.cpu()))\n",
    "                        all_true_labels = torch.cat((all_true_labels, sample['labels'].cpu()))\n",
    "\n",
    "\n",
    "            all_predictions = all_predictions.data.numpy()\n",
    "            all_true_labels = all_true_labels.data.numpy()\n",
    "\n",
    "            all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "\n",
    "            train_stats[-1]['val_f1_scores'][-1][-1] = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "            \n",
    "            \n",
    "#             print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; validation F1 score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "#                                                                              train_stats[-1]['train_losses'][-1][-1], \n",
    "#                                                                              train_stats[-1]['valid_losses'][-1][-1], \n",
    "#                                                                              train_stats[-1]['val_f1_scores'][-1][-1],\n",
    "#                                                                             (epochs - i)*(time.time() - start)/(i+1)))\n",
    "            train_stats[-1]['train_losses'][-1].append(0)\n",
    "            train_stats[-1]['valid_losses'][-1].append(0)\n",
    "            train_stats[-1]['val_f1_scores'][-1].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for vs, s in zip(vocab_sizes, train_stats):\n",
    "    data.append([vs, np.mean(np.array(s['val_f1_scores'])[:, -6:-1]), np.std(np.array(s['val_f1_scores'])[:, -6:-1])])\n",
    "    \n",
    "data = np.array(data)\n",
    "df = pd.DataFrame(data, columns=['vocab_sizes', 'mean f1 score', 'std'])\n",
    "df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = ClassifierBinary(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model_tmp.forward(tmp[1]['input'], tmp[1]['cl_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0936  0.9064\n",
       " 0.0510  0.9490\n",
       " 0.0435  0.9565\n",
       " 0.0318  0.9682\n",
       " 0.1032  0.8968\n",
       " 0.0978  0.9022\n",
       "[torch.cuda.FloatTensor of size 6x2 (GPU 0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 label_index,\n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=6, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        \n",
    "        super(ClassifierBinary, self).__init__()\n",
    "        \"\"\"\n",
    "            A model from paper \"A Convolutional Attention Model for Text Classification\" \n",
    "            by Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He \n",
    "            http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf\n",
    "            With added character level features\n",
    "            \n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_of_cl_features = num_of_cl_features\n",
    "        \n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim).cuda()\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
    "                 # // 2 is because we would like to concat hidden states, \n",
    "                # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2).cuda()\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1).cuda()\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 2).cuda()\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2)\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1)\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 2)\n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if(self.use_cuda):\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda(), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq, cl_features=None):\n",
    "        embed = self.embeddings(input_seq)\n",
    "        output, _ = self.lstm(embed, self.hidden)\n",
    "        \n",
    "        conv_out = self.conv(embed.permute(0, 2, 1))\n",
    "        \n",
    "        attention_tensor = torch.mean(conv_out, dim=1)\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        \n",
    "        if(cl_features is not None and self.num_of_cl_features == cl_features.data.shape[1]):\n",
    "            features = torch.cat((features, cl_features), dim=1)\n",
    "        elif(cl_features is not None and self.num_of_cl_features != cl_features.data.shape[1]):\n",
    "            print(\"\"\"Recieved unexpected number of character level features. \n",
    "                     Model expected to recieve {} features, but received {}. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features, cl_features.data.shape[1]))\n",
    "            raise ValueError()\n",
    "        elif(cl_features is None and self.num_of_cl_features > 0):\n",
    "            print(\"\"\"Model expected to recieve {} features, but received None. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features))\n",
    "            raise ValueError()\n",
    "            \n",
    "        predictions = nn.functional.softmax(self.linear_final(features), dim=1)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def train_ (self, \n",
    "               n_train_batches=2000,\n",
    "               n_valid_batches=500,\n",
    "               lr = 1e-3,\n",
    "               weight_decay = 1e-5,\n",
    "               epochs = 15):\n",
    "        \n",
    "        self.dataset = ToxicTextsDatasetBinary(self.label_index, \n",
    "                                          n_train_batches=n_train_batches, \n",
    "                                          n_valid_batches=n_valid_batches)\n",
    "                   \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if(self.use_cuda): loss_function = nn.CrossEntropyLoss().cuda()\n",
    "        else: loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#         train_stats = {'train_losses':[], 'valid_losses':[], 'val_f1_scores':[]}\n",
    "        \n",
    "#         train_stats['train_losses'].append(0)\n",
    "#         train_stats['valid_losses'].append(0)\n",
    "#         train_stats['val_f1_scores'].append(0)\n",
    "\n",
    "#         start = time.time()\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "#             all_predictions = torch.zeros(1, 1)\n",
    "#             all_true_labels = torch.zeros(1, 1)\n",
    "\n",
    "#             for mode in ['train', 'valid']:\n",
    "#                 dataset.mode = mode\n",
    "            self.dataset.shuffle()\n",
    "            for sample in self.dataset:\n",
    "#                 if(mode == 'train'):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                self.init_hidden()\n",
    "\n",
    "                pred = self.forward(sample['input'], sample['cl_features'])\n",
    "\n",
    "                loss = loss_function(pred, sample['labels'])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#                 train_stats['train_losses'][-1] += loss.data[0]\n",
    "#                     else:\n",
    "#                         self.init_hidden()\n",
    "\n",
    "#                         pred = self.forward(sample['input'], sample['cl_features'])\n",
    "#                         train_stats['valid_losses'][-1] += loss_function(pred, sample['labels']).data[0]\n",
    "\n",
    "#                         _, pred = torch.max(pred.data, 1)\n",
    "#                         all_predictions = torch.cat((all_predictions, torch.FloatTensor(pred.cpu().numpy())))\n",
    "#                         all_true_labels = torch.cat((all_true_labels, torch.FloatTensor(\n",
    "#                                                                         sample['labels'].data.cpu().numpy())))\n",
    "\n",
    "\n",
    "#             all_predictions = all_predictions.numpy()\n",
    "#             all_true_labels = all_true_labels.numpy()\n",
    "\n",
    "#             all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "\n",
    "#             train_stats['val_f1_scores'][-1] = f1_score(all_true_labels, all_predictions)\n",
    "            \n",
    "            \n",
    "#             print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; validation F1 score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "#                                                                              train_stats['train_losses'][-1], \n",
    "#                                                                              train_stats['valid_losses'][-1], \n",
    "#                                                                              train_stats['val_f1_scores'][-1],\n",
    "#                                                                             (epochs - i)*(time.time() - start)/(i+1)))\n",
    "#             train_stats['train_losses'].append(0)\n",
    "#             train_stats['valid_losses'].append(0)\n",
    "#             train_stats['val_f1_scores'].append(0)\n",
    "        \n",
    "#         return train_stats\n",
    "    def predict(self, X):\n",
    "        \n",
    "        code, cl_features = self.dataset.encode(X)\n",
    "        \n",
    "        Input = Variable(torch.LongTensor(np.array(codes)), \n",
    "                    requires_grad=False).cuda(),\n",
    "        \n",
    "        cl_features = Variable(torch.FloatTensor(np.array(cl_features)), \n",
    "                    requires_grad=False).cuda()\n",
    "        \n",
    "        pred = self.forward(Input, cl_features)\n",
    "                       \n",
    "#         _, pred = torch.max(pred.data, 1)\n",
    "        \n",
    "        return pred.data[:, 1]\n",
    "                               \n",
    "class EnsembleBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=6, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        super(EnsembleBinary, self).__init__()\n",
    "        \n",
    "        self.classifier1 = ClassifierBinary(0, vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier2 = ClassifierBinary(1, vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier3 = ClassifierBinary(2, vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier4 = ClassifierBinary(3, vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier5 = ClassifierBinary(4, vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        self.classifier6 = ClassifierBinary(5, vocab_size, embedding_dim, hidden_dim, batch_size, conv_channels, use_cuda, num_of_cl_features)\n",
    "        \n",
    "    def train_(self, \n",
    "           n_train_batches=2000,\n",
    "           n_valid_batches=500,\n",
    "           lr = 1e-3,\n",
    "           weight_decay = 1e-5,\n",
    "           epochs = 15):\n",
    "\n",
    "        train_stats1 = self.classifier1.train_(n_train_batches, n_valid_batches, lr, weight_decay, 3)\n",
    "        train_stats2 = self.classifier2.train_(n_train_batches, n_valid_batches, lr, weight_decay, 3)\n",
    "        train_stats3 = self.classifier3.train_(n_train_batches, n_valid_batches, lr, weight_decay, 3)\n",
    "        train_stats4 = self.classifier4.train_(n_train_batches, n_valid_batches, lr, weight_decay, 3)\n",
    "        train_stats5 = self.classifier5.train_(n_train_batches, n_valid_batches, lr, weight_decay, 4)\n",
    "        train_stats6 = self.classifier6.train_(n_train_batches, n_valid_batches, lr, weight_decay, 5)\n",
    "\n",
    "        return train_stats1, train_stats2, train_stats3, train_stats4, train_stats5, train_stats6\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        return torch.stack((self.classifier1.predict(X), \n",
    "                            self.classifier2.predict(X),\n",
    "                            self.classifier3.predict(X),\n",
    "                            self.classifier4.predict(X), \n",
    "                            self.classifier5.predict(X),\n",
    "                            self.classifier6.predict(X))).transpose(0, 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble():\n",
    "    def __init__(self, n_classifiers_per_label=1):\n",
    "        \n",
    "        self.classifiers = []\n",
    "        \n",
    "        for i in range(n_classifiers_per_label):\n",
    "            self.classifiers.append(EnsembleBinary())\n",
    "            self.classifiers[-1].train_()\n",
    "            print('trained {}-th ensemble of binary classifiers!'.format(i))\n",
    "\n",
    "            del self.classifiers[-1].classifier1.dataset.train_dataset\n",
    "            del self.classifiers[-1].classifier2.dataset.train_dataset\n",
    "            del self.classifiers[-1].classifier3.dataset.train_dataset\n",
    "            del self.classifiers[-1].classifier4.dataset.train_dataset\n",
    "            del self.classifiers[-1].classifier5.dataset.train_dataset\n",
    "            del self.classifiers[-1].classifier6.dataset.train_dataset\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for classifier in self.classifiers:\n",
    "            predictions.append(classifier.predict(X).numpy())\n",
    "        \n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ToxicTextsDataset(n_train_batches=0, n_test_batches=0, n_valid_batches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-215ab6985b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'some text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-2a0407dcf091>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-45a8a756c43b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         return torch.stack((self.classifier1.predict(X), \n\u001b[0m\u001b[1;32m    229\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-45a8a756c43b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         Input = Variable(torch.LongTensor(np.array(codes)), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "E.predict('some text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained 0-th ensemble of binary classifiers!\n"
     ]
    }
   ],
   "source": [
    "E = Ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.zeros(3), torch.ones(3), torch.ones(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(3, 2)\n",
    "for i in a:\n",
    "    t += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.mean received an invalid combination of arguments - got (list), but expected one of:\n * (torch.FloatTensor source)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m)\n * (torch.FloatTensor source, int dim)\n * (torch.FloatTensor source, int dim, bool keepdim)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-8ac49bb4b0bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: torch.mean received an invalid combination of arguments - got (list), but expected one of:\n * (torch.FloatTensor source)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m)\n * (torch.FloatTensor source, int dim)\n * (torch.FloatTensor source, int dim, bool keepdim)\n"
     ]
    }
   ],
   "source": [
    "torch.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (t < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_valid_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-079212f71108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsembleBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_valid_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_valid_batches'"
     ]
    }
   ],
   "source": [
    "EB = EnsembleBinary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000; train loss = 1235.71; validation loss = 0.00; validation F1 score = 0.00; ETA = 259 s\n",
      "Epoch 001; train loss = 1141.01; validation loss = 0.00; validation F1 score = 0.00; ETA = 232 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-a9a341fc8cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_valid_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-4ae3e8449692>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, n_train_batches, n_valid_batches, lr, weight_decay, epochs)\u001b[0m\n\u001b[1;32m    200\u001b[0m            epochs = 15):\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mtrain_stats1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mtrain_stats2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mtrain_stats3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-4ae3e8449692>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, n_train_batches, n_valid_batches, lr, weight_decay, epochs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                         \u001b[0mtrain_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EB.train_(n_valid_batches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ToxicTextsDatasetBinary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.LongTensor of size 6]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['labels'].data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cl_features': Variable containing:\n",
       "  37.0000   2.9000   0.0270   0.2432\n",
       "  51.0000   4.3000   0.0000   0.1765\n",
       "  47.0000   4.0000   0.0213   0.1915\n",
       "  47.0000   4.1000   0.0213   0.2340\n",
       "  44.0000   3.7000   0.0682   0.2273\n",
       "  43.0000   3.3000   0.0465   0.2558\n",
       " [torch.cuda.FloatTensor of size 6x4 (GPU 0)], 'input': Variable containing:\n",
       "  1786   457   177    33    21   441    11    61   101     0\n",
       "   653  1872   192    44    76    29   125    12   555     2\n",
       "  1034  1999   125    74   753  1034  1999     2  1999     2\n",
       "   568    19     3   211  1999   300     3    24   689    63\n",
       "   304    99     7    24    47     2    29    71    31     2\n",
       "   227   125  1872   277   142    76    38     7    72     2\n",
       " [torch.cuda.LongTensor of size 6x10 (GPU 0)], 'labels': Variable containing:\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " [torch.cuda.LongTensor of size 6 (GPU 0)]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = EB.classifier1.forward(data[0]['input'], data[0]['cl_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, pred = torch.max(pred.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, pred1 = torch.max(pred1.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = EB.classifier2.forward(data[0]['input'], data[0]['cl_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  1.0000\n",
       "  1.0000\n",
       "  0.9988\n",
       "  1.0000\n",
       "  0.9613\n",
       "  0.9999\n",
       " [torch.cuda.FloatTensor of size 6 (GPU 0)], \n",
       "  0\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  0\n",
       " [torch.cuda.LongTensor of size 6 (GPU 0)])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(pred.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0000  0.0000\n",
       " 1.0000  0.0000\n",
       " 0.0012  0.9988\n",
       " 0.0000  1.0000\n",
       " 0.0387  0.9613\n",
       " 0.9999  0.0001\n",
       "[torch.cuda.FloatTensor of size 6x2 (GPU 0)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1\n",
       " 0  0\n",
       " 1  1\n",
       " 1  0\n",
       " 1  0\n",
       " 0  1\n",
       "[torch.LongTensor of size 6x2]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((pred.cpu(), pred1.cpu())).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 6]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(torch.LongTensor([1]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    1\n",
       "    0\n",
       "    0\n",
       "[torch.FloatTensor of size 7x1]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((z, torch.FloatTensor(pred.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.LongTensor constructor received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * no arguments\n * (int ...)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongTensor viewed_tensor)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.Size size)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongStorage data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (Sequence data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-701394108fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.LongTensor constructor received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * no arguments\n * (int ...)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongTensor viewed_tensor)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.Size size)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.LongStorage data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (Sequence data)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor(torch.ones(2, 3))\n",
    "b = torch.LongTensor(torch.zeros(4, 3))\n",
    "torch.cat((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 10))\n",
    "# for i in range(9):\n",
    "#     x = np.arange(10)\n",
    "#     y = np.mean(np.array(train_stats[i]['val_f1_scores'])[:, :-1], axis=0)\n",
    "#     std = np.std(np.array(train_stats[i]['val_f1_scores'])[:, :-1], axis=0)\n",
    "        \n",
    "#     plt.errorbar(x, y, yerr=std, label='lr = {}, wd = {}'.format(train_stats[i]['lr'], train_stats[i]['weight_decay']))\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# fig = plt.figure(figsize=(9, 9))\n",
    "# ax = fig.gca(projection='3d')\n",
    "\n",
    "# ax.view_init(30, 90)\n",
    "\n",
    "# xx, yy = np.log(np.meshgrid(learning_rates, weight_decays))\n",
    "\n",
    "# tmp_lr = {0.01:0, 0.001:1, 0.0001:2}\n",
    "# tmp_wd = {0.001:0, 0.0001:1, 0.00001:2}\n",
    "\n",
    "# z = np.zeros((3, 3))\n",
    "# z_std = np.zeros((3, 3))\n",
    "# for s in train_stats:\n",
    "#     z[tmp_lr[s['lr']], tmp_wd[s['weight_decay']]] = np.mean(np.array(s['val_f1_scores'])[:, -6:-1])\n",
    "#     z_std[tmp_lr[s['lr']], tmp_wd[s['weight_decay']]] = np.std(np.array(s['val_f1_scores'])[:, -6:-1])\n",
    "\n",
    "# ax.plot_surface(xx, yy, z, cmap=plt.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.vstack((np.exp(xx).flatten(), np.exp(yy).flatten(), z.flatten(), z_std.flatten())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data, columns=['learning rate', 'weight decay', 'mean f1 score', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
