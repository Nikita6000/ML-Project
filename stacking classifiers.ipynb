{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as td\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ToxicTextsDatasetBinary(td.Dataset):\n",
    "    def __init__(self, label_index,\n",
    "                       data_path='train.csv', \n",
    "                       n_train_batches=4000, \n",
    "                       n_test_batches=4000,\n",
    "                       n_valid_batches=1600,\n",
    "                       separate_test_and_valid=True,\n",
    "                       test_size=0.,\n",
    "                       valid_size=0.3,\n",
    "                       batch_size=6, \n",
    "                       vocab_size=2000,\n",
    "                       mode='train',\n",
    "                       random_seed=None,\n",
    "                       verbose=0,\n",
    "                       use_cuda = True):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            n_train_batches - int, number of batches to be drawn from data for training\n",
    "            n_test_batches -  int, number of batches to be drawn from data for testing\n",
    "            n_valid_batches -  int, number of batches to be drawn from data for validation\n",
    "            separate_test_and_valid - bool, wherever to draw training, testing and validation \n",
    "                                      from all data or from separated parts of data (a chance \n",
    "                                      of intersection between training, testing and validation \n",
    "                                      data if False)\n",
    "            test_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                        dataset for testing. Not aplicable if separate_test_and_valid=False\n",
    "            valid_size - float from [0, 1], a portion of initial data reserved for creating \n",
    "                         dataset for validation. Not aplicable if separate_test_and_valid=False\n",
    "            batch_size - int, number of samples in one minibatch\n",
    "            vocab_size - int, number of unique tokens to save and embed. Saved [vocab_size] \n",
    "                         most frequently encountered tokens, all others will be encoded as \n",
    "                         UNKNOWN token\n",
    "            mode = string, one from ['train', 'test', 'valid']. Determinedes from which dataset \n",
    "                    will be returned sample on ToxicTextsDataset[i]\n",
    "            verbose - int, 0 for no printed info, 1 for minimum info, 2 for maximum info\n",
    "            \n",
    "        \"\"\"\n",
    "        super(ToxicTextsDatasetBinary, self).__init__()\n",
    "        \n",
    "        self.n_train_batches = n_train_batches\n",
    "        self.n_test_batches = n_test_batches\n",
    "        self.n_valid_batches = n_valid_batches\n",
    "        self.separate_test_and_valid = separate_test_and_valid\n",
    "        self.test_size = test_size\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(random_seed != None):\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        if(verbose): print('Downloading data from ' + data_path + '... ', end='')\n",
    "        # read csv file\n",
    "        df = pd.read_csv(data_path)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        # separate text from class labels\n",
    "        X = np.array(df.iloc[:, 1])\n",
    "        y = np.array(df.iloc[:, 2+label_index])\n",
    "        \n",
    "        if(verbose): print('Generating vocabulary... ', end='')\n",
    "        # generating vocabulary of tokens\n",
    "        self.CreateTokenVocab(X, y)\n",
    "        if(verbose): print('Completed')\n",
    "        \n",
    "        if(separate_test_and_valid == True):\n",
    "            # split data for\n",
    "            X_train, X, y_train, y = train_test_split(X, y, test_size=valid_size + test_size)\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X_train, y_train, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(test_size != 0 and valid_size != 0):\n",
    "                X_test, X_valid, y_test, y_valid = train_test_split(X, y, \n",
    "                                                    test_size=valid_size/(test_size+valid_size))\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                    \n",
    "            elif(test_size == 0):\n",
    "                X_valid = X\n",
    "                y_valid = y\n",
    "                \n",
    "                if(verbose): print('Creating validation dataset... ', end='')\n",
    "                self.valid_dataset = self.CreateBalancedDataset(X_valid, y_valid, n_valid_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.test_dataset = []              \n",
    "                    \n",
    "            elif(valid_size == 0):\n",
    "                X_test = X\n",
    "                y_test = y\n",
    "                \n",
    "                if(verbose): print('Creating test dataset... ', end='')\n",
    "                self.test_dataset = self.CreateBalancedDataset(X_test, y_test, n_test_batches)\n",
    "                if(verbose): print('Completed')\n",
    "                \n",
    "                self.valid_dataset = []            \n",
    "                \n",
    "        elif(separate_test_and_valid == False):\n",
    "            \n",
    "            if(verbose): print('Creating train dataset... ', end='')\n",
    "            self.train_dataset = self.CreateBalancedDataset(X, y, n_train_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating test dataset... ', end='')\n",
    "            self.test_dataset = self.CreateBalancedDataset(X, y, n_test_batches)\n",
    "            if(verbose): print('Completed')\n",
    "            \n",
    "            if(verbose): print('Creating validation dataset... ', end='')\n",
    "            self.valid_dataset = self.CreateBalancedDataset(X, y, n_valid_batches)\n",
    "            if(verbose): print('Completed')\n",
    "                    \n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" function that splits text into tokens and returns a list of encodings for \n",
    "            each token \n",
    "                INPUT: text - python string\n",
    "                OUTPUT: codes - list of integers, \n",
    "                        cl_features - list of floats (character level features)\n",
    "        \"\"\"\n",
    "        tokens = self.Smart_Split(text)\n",
    "        codes = []\n",
    "        cl_features = self.ComputeCharacterLevelFeatures(text)\n",
    "        for token in tokens:\n",
    "            if(self.word_to_id.get(token) != None):\n",
    "                codes.append(self.word_to_id[token])\n",
    "            else:\n",
    "                codes.append(self.vocab_size - 1) # UNKNOWN token\n",
    "        return codes, cl_features\n",
    "    \n",
    "    def ComputeCharacterLevelFeatures(self, text):\n",
    "        \"\"\"This function computes a character level features \n",
    "           INPUT: text - a python string\n",
    "           OUTPUT: cl_features - a list of floats\n",
    "               \n",
    "               cl_features[0] - lenght of text\n",
    "               cl_features[1] - mean of lenghts of all tokens in text\n",
    "               cl_features[2] - ratio of capital letters in text\n",
    "               cl_features[3] - ratio of non-letter symbols in text\n",
    "        \"\"\"\n",
    "        text_len = float(len(text))\n",
    "        \n",
    "        cl_features = [\n",
    "            text_len,\n",
    "            np.mean([len(token) for token in self.Smart_Split(text)]),\n",
    "            len(re.findall(r'[A-Z]', text)) / text_len,\n",
    "            (1. - len(re.findall(r'[a-zA-Z]', text)) / text_len)\n",
    "        ]\n",
    "        \n",
    "        return cl_features\n",
    "    \n",
    "    def CreateBalancedDataset(self, X, y, n_batches):\n",
    "        \"\"\"This functions returns a balanced dataset (a list of batched samples with \n",
    "           corresponding labels). Produced dataset is drawn with repetition from initial data, \n",
    "           and therefore can contain duplicates Depending on n_batches, it will do either \n",
    "           undersampling, oversampling or combination of both\n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed text \n",
    "                     as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels (label != 0 is assumed to be \"interesting\" )\n",
    "                 n_batches - integer, number of batches in dataset (so the number of samples \n",
    "                             in dataset is equal to n_batches * batch_size = len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th batch \n",
    "                            of inputs and dataset[i]['labels'] - corresponding batch of labels\"\"\"\n",
    "        dataset = []\n",
    "        n_subbatches = n_batches // 2\n",
    "        \n",
    "        mask = (y == 1)\n",
    "        dataset += self.CreateDatasetFromXY(X[mask], y[mask], n_subbatches)\n",
    "        \n",
    "        mask = (y == 0)\n",
    "        dataset += self.CreateDatasetFromXY(X[mask], y[mask], n_subbatches)\n",
    "        \n",
    "        return shuffle(dataset)\n",
    "    \n",
    "    def CreateDatasetFromXY(self, X, y, n_batches):\n",
    "        \"\"\"\n",
    "        This functions constructs and returns a dataset (a list of batched samples \n",
    "        with corresponding labels). \n",
    "        \n",
    "          INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                     text as elements\n",
    "                 y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                     classification labels\n",
    "                 n_batches - integer, number of batches in dataset (so the number \n",
    "                             of samples in dataset is equal to n_batches * batch_size = \n",
    "                             len(dataset) * batch_size)\n",
    "          OUTPUT:\n",
    "                  dataset - list of dictionaries where dataset[i]['input'] is a i-th \n",
    "                            batch of inputs and dataset[i]['labels'] - corresponding \n",
    "                            batch of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        # we sort our samples on the lenght of the text (in the number of tokens) and \n",
    "        # place texts of the same lenght in the same position in this dictionary. \n",
    "        # This can be also viewed as a hash-table\n",
    "        Len_table = dict()\n",
    "        for i in range(len(X)):\n",
    "            codes, cl_features = self.encode(X[i])\n",
    "            if(Len_table.get(len(codes)) != None):\n",
    "                Len_table[len(codes)].append((codes, cl_features, y[i]))\n",
    "            else: \n",
    "                Len_table[len(codes)] = [(codes, cl_features, y[i])]\n",
    "        \n",
    "        # we have different number of samples of different lenght. There is a lot more \n",
    "        # samples of lenght ~10-50 tokens and much smaller number of samples of lenght \n",
    "        # 100+ tokens. Now we will get a distribution of number of samples:\n",
    "        dist = np.array([[i, len(Len_table[i])] for i in Len_table.keys()])\n",
    "        # here dist[i, 0] is some lenght of sample we encountered in dataset\n",
    "        # and dist[i, 1] is a number of samples of that lenght \n",
    "        \n",
    "        p = dist[:, 1] / np.sum(dist[:, 1])\n",
    "        \n",
    "        # we will construct actual dataset, randomly drawing samples from that distribution:\n",
    "        dataset = []\n",
    "        for _ in range(n_batches):\n",
    "            i = np.random.choice(dist[:, 0], p=p)\n",
    "            sample_indices = np.random.randint(0, len(Len_table[i]), self.batch_size)\n",
    "            # it took me some time to figure out correct transformation from mess of \n",
    "            # lists and numpy array to torch tensor :)\n",
    "            if(self.use_cuda):\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False).cuda(),\n",
    "                         'labels':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False).cuda()}\n",
    "            else:\n",
    "                batch = {'input':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 0].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'cl_features':Variable(torch.FloatTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 1].tolist())), \n",
    "                    requires_grad=False),\n",
    "                         'labels':Variable(torch.LongTensor(\n",
    "                    np.array(np.array(Len_table[i])[sample_indices][:, 2].tolist())), \n",
    "                    requires_grad=False)}\n",
    "                \n",
    "            dataset.append(batch)        \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def CreateTokenVocab(self, X, y):\n",
    "        '''This function generates a word_to_id dictionary we use for encoding text\n",
    "        \n",
    "            INPUT: X - one dimensional np.array of shappe (n_samples, ) with unparsed \n",
    "                       text as elements\n",
    "                   y - two dimensional np.array of shape (n_samples, n_labels) with \n",
    "                       classification labels (label != 0 is assumed to be \"interesting\" - \n",
    "                       we prioretize tokens encoundered in examples with at least one label = 1)\n",
    "        \n",
    "        '''\n",
    "        token_freq = dict()\n",
    "\n",
    "        # firstly we exctract all tokens we see in positivly labeled samples\n",
    "        X_relevant = X[y == 1] \n",
    "        X_relevant += shuffle(X[y == 0])[:len(X_relevant)] \n",
    "        # we add random portion of \"all-negative\" data of equal size \n",
    "         \n",
    "        for text in X_relevant:\n",
    "            tokens = self.Smart_Split(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if(token_freq.get(token) == None):\n",
    "                    token_freq[token] = 1\n",
    "                else: token_freq[token] += 1\n",
    "\n",
    "        tokens = sorted(token_freq, key=token_freq.get)[::-1]\n",
    "\n",
    "        # secondly, we assign id's to the most frequently encountered tokens in positivly \n",
    "        # classified samples\n",
    "        self.word_to_id = dict()\n",
    "        for i in range(self.vocab_size - 1):\n",
    "            self.word_to_id[tokens[i]] = i\n",
    "\n",
    "        # finally, we would like to find very similar tokens and assign to them the \n",
    "        # same id (those are mainly misspells and parsing \n",
    "        # innacuracies. For example 'training', 'traning', 'trainnin', 'training\"' and so on)\n",
    "        vec = TfidfVectorizer()\n",
    "        vec_tokens = vec.fit_transform(tokens)\n",
    "        same_tokens = ((vec_tokens * vec_tokens.T) > 0.99)\n",
    "        rows, cols = same_tokens.nonzero()\n",
    "\n",
    "        for token_pair in zip(rows, cols):\n",
    "            if(token_pair[0] > self.vocab_size):\n",
    "                break\n",
    "            if(token_pair[0] <= token_pair[1]):\n",
    "                continue\n",
    "            else:\n",
    "                self.word_to_id[tokens[token_pair[1]]] = token_pair[0]\n",
    "    \n",
    "    def Smart_Split(self, text):\n",
    "        \"\"\"Parsing function \n",
    "            INPUT: text - python string with any text\n",
    "            OUTPUT: list of strings, containing tokens\n",
    "        \"\"\"\n",
    "        out = text.strip().lower().replace('\\n', ' ')\n",
    "        out = out.replace(',', ' , ').replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ')\n",
    "        out = out.replace(')', ' ) ').replace('(', ' ( ').replace(':', ' : ').replace(';', ' ; ')\n",
    "        out = out.replace('.  .  .', '...')\n",
    "        return out.split()\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if(self.mode == 'train'):\n",
    "            return self.train_dataset[i]\n",
    "        elif(self.mode == 'test'):\n",
    "            return self.test_dataset[i]\n",
    "        elif(self.mode == 'valid'):\n",
    "            return self.valid_dataset[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.mode == 'train'):\n",
    "            return len(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            return len(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            return len(self.valid_dataset)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"shuffles dataset, corresponding to current mode\"\"\"\n",
    "        if(self.mode == 'train'):\n",
    "            self.train_dataset = shuffle(self.train_dataset)\n",
    "        elif(self.mode == 'test'):\n",
    "            self.test_dataset = shuffle(self.test_dataset)\n",
    "        elif(self.mode == 'valid'):\n",
    "            self.valid_dataset = shuffle(self.valid_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 label_index,\n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=6, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4):\n",
    "        \n",
    "        super(ClassifierBinary, self).__init__()\n",
    "        \"\"\"\n",
    "            A model from paper \"A Convolutional Attention Model for Text Classification\" \n",
    "            by Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He \n",
    "            http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf\n",
    "            With added character level features\n",
    "            \n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_channels = conv_channels\n",
    "        self.use_cuda = use_cuda\n",
    "        self.num_of_cl_features = num_of_cl_features\n",
    "        \n",
    "        self.label_index = label_index\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim).cuda()\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True).cuda()\n",
    "                 # // 2 is because we would like to concat hidden states, \n",
    "                # calculated from both sides of LSTM and aquire exactly hidden_dim\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2).cuda()\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1).cuda()\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 2).cuda()\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.conv = nn.Conv1d(in_channels=embedding_dim, \n",
    "                                  out_channels=conv_channels, \n",
    "                                  kernel_size=5, \n",
    "                                  padding=2)\n",
    "    \n",
    "            self.linear = nn.Linear(conv_channels, 1)\n",
    "            self.linear_final = nn.Linear(hidden_dim + num_of_cl_features, 2)\n",
    "            \n",
    "        self.init_hidden()\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if(self.use_cuda):\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda(), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            self.hidden = (Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)), \n",
    "                           Variable(torch.zeros(2, \n",
    "                                                self.batch_size, \n",
    "                                                self.hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, input_seq, cl_features=None, batched=True):\n",
    "        \n",
    "        embed = self.embeddings(input_seq)\n",
    "            \n",
    "        if(batched):\n",
    "            output, _ = self.lstm(embed, self.hidden)\n",
    "        else:\n",
    "            output, _ = self.lstm(torch.cat((embed, )*6), self.hidden)\n",
    "        \n",
    "        output = output[:1, :, :]\n",
    "        conv_out = self.conv(embed.permute(0, 2, 1))\n",
    "        \n",
    "        attention_tensor = torch.mean(conv_out, dim=1)\n",
    "        \n",
    "        features = torch.sum(output * attention_tensor.resize(attention_tensor.data.shape[0], attention_tensor.data.shape[1], 1), dim=1)\n",
    "        \n",
    "        if(cl_features is not None and self.num_of_cl_features == cl_features.data.shape[1]):\n",
    "            features = torch.cat((features, cl_features), dim=1)\n",
    "        elif(cl_features is not None and self.num_of_cl_features != cl_features.data.shape[1]):\n",
    "            print(\"\"\"Recieved unexpected number of character level features. \n",
    "                     Model expected to recieve {} features, but received {}. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features, cl_features.data.shape[1]))\n",
    "            raise ValueError()\n",
    "        elif(cl_features is None and self.num_of_cl_features > 0):\n",
    "            print(\"\"\"Model expected to recieve {} features, but received None. \n",
    "                     Check model constructor or sample passed in forward\"\"\".format(self.num_of_cl_features))\n",
    "            raise ValueError()\n",
    "            \n",
    "        predictions = nn.functional.softmax(self.linear_final(features), dim=1)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def train_ (self, \n",
    "               n_train_batches=2000,\n",
    "               n_valid_batches=500,\n",
    "               lr = 1e-3,\n",
    "               weight_decay = 1e-5,\n",
    "               epochs = 15,\n",
    "               fitted_clf = None):\n",
    "        \n",
    "        self.dataset = ToxicTextsDatasetBinary(self.label_index, \n",
    "                                          n_train_batches=n_train_batches, \n",
    "                                          n_valid_batches=n_valid_batches,\n",
    "                                              use_cuda = self.use_cuda)\n",
    "                   \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if(self.use_cuda): loss_function = nn.CrossEntropyLoss().cuda()\n",
    "        else: loss_function = nn.CrossEntropyLoss()\n",
    "            \n",
    "            \n",
    "        train_stats = {'train_losses':[], 'valid_losses':[], 'val_f1_scores':[]}\n",
    "        \n",
    "        train_stats['train_losses'].append(0)\n",
    "        train_stats['valid_losses'].append(0)\n",
    "        train_stats['val_f1_scores'].append(0)\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            all_predictions = torch.zeros(1)\n",
    "            all_true_labels = torch.zeros(1)\n",
    "\n",
    "            for mode in ['train', 'valid']:\n",
    "                dataset.mode = mode\n",
    "                self.dataset.shuffle()\n",
    "                for sample in self.dataset:\n",
    "                    if(mode == 'train'):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        self.init_hidden()\n",
    "                        X = sample['input']\n",
    "                        features = sample['cl_features']\n",
    "\n",
    "                        if fitted_clf is not None:\n",
    "                            for clf in fitted_clf:\n",
    "                                prediction = clf.forward(X, features).data[:, 1]\n",
    "                                features = torch.cat((features, Variable(prediction.expand((1,batch_size)).transpose(0,1))), \n",
    "                                                     dim =1)\n",
    "\n",
    "                        pred = self.forward(X, features)\n",
    "                        loss = loss_function(pred, sample['labels'])\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_stats['train_losses'][-1] += loss.data[0]\n",
    "                    else:\n",
    "                        self.init_hidden()\n",
    "                        X = sample['input']\n",
    "                        features = sample['cl_features']\n",
    "\n",
    "                        if fitted_clf is not None:\n",
    "                            for i, clf in enumerate(fitted_clf):\n",
    "                                prediction = clf.forward(X, features).data[:, 1]\n",
    "                                features = torch.cat((features, Variable(prediction.expand((1,batch_size)).transpose(0,1))), \n",
    "                                                     dim =1)\n",
    "                        pred = self.forward(X, features)\n",
    "                        train_stats['valid_losses'][-1] += loss_function(pred, sample['labels']).data[0]\n",
    "\n",
    "                        _, pred = torch.max(pred.data, 1)\n",
    "                        \n",
    "                        all_predictions = torch.cat((all_predictions, torch.FloatTensor(pred.cpu().numpy())))\n",
    "                        all_true_labels = torch.cat((all_true_labels, torch.FloatTensor(\n",
    "                                                                        sample['labels'].data.cpu().numpy())))\n",
    "\n",
    "\n",
    "            all_predictions = all_predictions.numpy()\n",
    "            all_true_labels = all_true_labels.numpy()\n",
    "\n",
    "            all_predictions = (all_predictions - 0.5 > 0).astype(int)\n",
    "\n",
    "            train_stats['val_f1_scores'][-1] = roc_auc_score(all_true_labels, all_predictions)\n",
    "            \n",
    "            \n",
    "            print('Epoch {:03d}; train loss = {:4.2f}; validation loss = {:2.2f}; validation roc_auc_score = {:0.2f}; ETA = {:3.0f} s'.format(i, \n",
    "                                                                             train_stats['train_losses'][-1], \n",
    "                                                                             train_stats['valid_losses'][-1], \n",
    "                                                                             train_stats['val_f1_scores'][-1],\n",
    "                                                                            (epochs - i)*(time.time() - start)/(i+1)))\n",
    "            train_stats['train_losses'].append(0)\n",
    "            train_stats['valid_losses'].append(0)\n",
    "            train_stats['val_f1_scores'].append(0)\n",
    "        \n",
    "        return train_stats\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X, features=None, encoded=False, fitted_clf = None):\n",
    "        \n",
    "        if encoded:\n",
    "            if fitted_clf is not None:\n",
    "                for clf in fitted_clf:\n",
    "                    prediction = clf.forward(X, features).data[:, 1]\n",
    "                    features = torch.cat((features, Variable(prediction.expand((1,batch_size)).transpose(0,1))), dim =1)\n",
    "            Input = X\n",
    "            cl_features = features\n",
    "            pred = self.forward(Input, cl_features, batched=True)\n",
    "            \n",
    "        else:\n",
    "            codes, cl_features = self.dataset.encode(X)\n",
    "\n",
    "            if self.use_cuda:\n",
    "                Input = Variable(torch.LongTensor(np.array(codes)), \n",
    "                                 requires_grad=False).resize(1, len(codes)).cuda()\n",
    "                features = Variable(torch.FloatTensor(np.array(cl_features)), \n",
    "                                       requires_grad=False).resize(1, len(cl_features)).cuda()\n",
    "            else:\n",
    "                Input = Variable(torch.LongTensor(np.array(codes)), \n",
    "                                 requires_grad=False).resize(1, len(codes))\n",
    "                features = Variable(torch.FloatTensor(np.array(cl_features)), \n",
    "                                       requires_grad=False).resize(1, len(cl_features))\n",
    "            \n",
    "            if fitted_clf is not None:\n",
    "                for clf in fitted_clf:\n",
    "                    prediction = clf.forward(Input, features, batched=False).data[:, 1]\n",
    "                    features = torch.cat((features, Variable(prediction.expand((1,1)).transpose(0,1))), dim =1)\n",
    "            \n",
    "            cl_features = features\n",
    "            pred = self.forward(Input, cl_features, batched=False)\n",
    "                       \n",
    "#         _, pred = torch.max(pred.data, 1)\n",
    "        \n",
    "        return pred.data[:, 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainBinary(nn.Module):\n",
    "    def __init__(self, \n",
    "                 order = None,\n",
    "                 vocab_size=2000, \n",
    "                 embedding_dim = 100, \n",
    "                 hidden_dim=200, \n",
    "                 batch_size=6, \n",
    "                 conv_channels=32, \n",
    "                 use_cuda=True,\n",
    "                 num_of_cl_features=4,\n",
    "                 epochs = [3,3,3,3,4,5]):\n",
    "        super(ChainBinary, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if order is None:\n",
    "            self.order_ = np.random.permutation(range(6))\n",
    "        else:\n",
    "            self.order_ = order\n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.epochs_ = epochs\n",
    "        self.all_clfs = [] # All currently fitted binary classifiers\n",
    "        \n",
    "        self.all_clfs.append(ClassifierBinary(self.order_[0], vocab_size, embedding_dim, hidden_dim, \n",
    "                                     batch_size, conv_channels, self.use_cuda, num_of_cl_features))\n",
    "        \n",
    "        \n",
    "         \n",
    "    def train_(self, \n",
    "           n_train_batches=10,\n",
    "           n_valid_batches=0,\n",
    "           lr = 1e-3,\n",
    "           weight_decay = 1e-5):\n",
    "    \n",
    "        train_stat = []\n",
    "        print('Training binary classifier for target {}...'.format(self.order_[0]))\n",
    "        train_stat.append(self.all_clfs[0].train_(n_train_batches, n_valid_batches, lr, \n",
    "                                                  weight_decay, self.epochs_[self.order_[0]]))\n",
    "       \n",
    "        for i in range(1,6):\n",
    "            print('Training binary classifier for target  {}...'.format(self.order_[i]))\n",
    "            self.all_clfs.append(ClassifierBinary(self.order_[i], vocab_size, embedding_dim, hidden_dim, \n",
    "                                     batch_size, conv_channels, self.use_cuda, num_of_cl_features+i))\n",
    "            train_stat.append(self.all_clfs[-1].train_(n_train_batches, n_valid_batches, lr, weight_decay, \n",
    "                                                       self.epochs_[self.order_[i]], self.all_clfs[:-1]))\n",
    "            \n",
    "        print('Done!')    \n",
    "        return train_stat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        num = []\n",
    "        for i in range(6):\n",
    "            num.append(np.where(self.order_ == i)[0][0])\n",
    "        \n",
    "        return torch.stack((self.all_clfs[num[0]].predict(X, fitted_clf = self.all_clfs[:num[0]]), \n",
    "                            self.all_clfs[num[1]].predict(X, fitted_clf = self.all_clfs[:num[1]]),\n",
    "                            self.all_clfs[num[2]].predict(X, fitted_clf = self.all_clfs[:num[2]]),\n",
    "                            self.all_clfs[num[3]].predict(X, fitted_clf = self.all_clfs[:num[3]]),\n",
    "                            self.all_clfs[num[4]].predict(X, fitted_clf = self.all_clfs[:num[4]]),\n",
    "                            self.all_clfs[num[5]].predict(X, fitted_clf = self.all_clfs[:num[5]]))).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnsembleChains():\n",
    "    def __init__(self, n_chains=10, use_cuda = True, epochs = [3,3,3,3,4,5]):\n",
    "        \n",
    "        self.chains = []\n",
    "        self.use_cuda = use_cuda\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        for i in tqdm.tqdm_notebook(range(n_chains)):\n",
    "            \n",
    "            self.chains.append(ChainBinary(epochs = self.epochs, use_cuda = self.use_cuda))\n",
    "            self.chains[-1].train_()\n",
    "            #print('trained {}-th ensemble of binary classifiers!'.format(i))\n",
    "\n",
    "            del self.chains[-1].all_clfs[0].dataset.train_dataset\n",
    "            del self.chains[-1].all_clfs[1].dataset.train_dataset\n",
    "            del self.chains[-1].all_clfs[2].dataset.train_dataset\n",
    "            del self.chains[-1].all_clfs[3].dataset.train_dataset\n",
    "            del self.chains[-1].all_clfs[4].dataset.train_dataset\n",
    "            del self.chains[-1].all_clfs[5].dataset.train_dataset\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for chain in self.chains:\n",
    "            predictions.append(chain.predict(X).cpu().numpy()[0])\n",
    "        \n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC = EnsembleChains(use_cuda = True, epochs = [5,5,5,5,5,5], n_chains = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df_new = pd.DataFrame(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "for ID, line in tqdm.tqdm_notebook(zip(df.iloc[:, 0].astype(str), df.iloc[:, 1]), total=len(df)):\n",
    "    if(line != '\\u2003'):\n",
    "        df_new.loc[ID] = EC.predict(line)[0]\n",
    "    else:\n",
    "        df_new.loc[ID] = [0., 0., 0., 0., 0., 0.]\n",
    "\n",
    "df_new.to_csv('test_predictions.csv')\n",
    "\n",
    "# DO NOT FORGET TO ADD 'id' TO FILE !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50045449,  0.68106574,  0.20780464,  0.62777185,  0.39182198,\n",
       "        0.31945559], dtype=float32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EC.predict('lol kek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
